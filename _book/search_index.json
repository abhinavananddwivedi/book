[["index.html", "Book: Statistical Finance and Computing Chapter 1 About", " Book: Statistical Finance and Computing Abhinav Anand 2021-08-19 Chapter 1 About This is a book-in-progress about ideas in statistical finance "],["introduction-to-basic-r.html", "Chapter 2 Introduction to basic R 2.1 Setup 2.2 Basics 2.3 Pipes 2.4 Data-types in R: Some Examples 2.5 Functions 2.6 Directory Management 2.7 RStudio Projects", " Chapter 2 Introduction to basic R 2.1 Setup The following discussion assumes we have donwloaded R and RStudio. The package gapminder needs to be installed prior to running the commands below. For downloading R, visit https://cran.r-project.org/ For downloading RStudio visit https://www.rstudio.com/ To install gapminder, type install.packages(\"gapminder\") in the RStudio console. 2.2 Basics The most basic way in which one can use R is as a calculator: (t &lt;- sin(pi/4)) #enclosing in parentheses prints output ## [1] 0.7071068 (2.342929*1.19483)/4.9802244 ## [1] 0.5621036 x &lt;- 3*10.4293939 round(x, digits = 4) ## [1] 31.2882 2.2.1 Remarks Note the use of &lt;- as opposed to =. In other programming languages, one uses the equality sign for assignment but in R the assignment operator is different. This is so because = is reserved for use in function arguments. (Example: round(x, digits = 4). Note however, that even if = is used in lieu of &lt;- all codes work okay. Confirm by computing, say (x = 3*10.4293939)) Some other useful operations: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10) #the sequence generator function ## [1] 1 2 3 4 5 6 7 8 9 10 R has several packages with in-built functions. In order to use them, one needs to declare their use. For our purposes, we will extensively use the suite of packages tidyverse written primarily by Hadley Wickham. library(tidyverse) 2.3 Pipes Consider the following problem: given a vector, we need to take its sine, then take the mean, then square it and then if the resulting number is more than 1, print more than 1, else print less than 1. We can do this in the following way: x &lt;- 1:100 #initial vector x_1 &lt;- sin(x) #note vectorized function x_2 &lt;- (mean(x_1))^2 #take mean, square if(x_2 &gt;= 1) #note the syntax for the if statement { print(&quot;more than 1&quot;) } else { print(&quot;less than 1&quot;) } ## [1] &quot;less than 1&quot; # or more succinctly in one line if(mean(sin(x))^2 &gt;= 1) { print(&quot;more than 1&quot;) } else { print(&quot;less than 1&quot;) } ## [1] &quot;less than 1&quot; However, there is another way to do this with pipes which can be read as the then operator. In fact, whenever we encounter the pipe symbol %&gt;%, without loss of generality, we can replace it with the word then. x_3 &lt;- x %&gt;% sin(.) %&gt;% mean(.) %&gt;% .^2 # read as: take x, then take sine # then take mean, then square if(x_3 &gt;= 1) { print(&quot;more than 10&quot;) } else { print(&quot;less than 1&quot;) } ## [1] &quot;less than 1&quot; Pipes are very powerful when expressing a sequence of operations. The pipe, %&gt;% comes from the magrittr package but tidyverse loads it automatically. 2.4 Data-types in R: Some Examples 2.4.1 Vectors There are two main types of vectors in R: atomic and non-atomic. Atomic vectors have components of the same type, say double, or integer or logical etc. These are also referred to as numeric vectors. By default all vectors in R are considered column-vectors. Non-atomic vectors (including lists) can contain heterogenous components. 2.4.2 Recycling There are no built-in scalars in R. Scalars are implemented as vectors of length 1. Most functions in R, hence, are vectorizedthey take vector arguments and operate on it component-wise. For example, for the vector x &lt;- 1:100, the function sin(x) produces each components sine, i.e., sin(1):sin(100). When we mix scalars and vectors, the scalars are automatically replicated to be the same size as the vector. For example: 1 + 1:10 ## [1] 2 3 4 5 6 7 8 9 10 11 #is the same operation as rep(1, 10) + 1:10 #note the highly useful rep() function ## [1] 2 3 4 5 6 7 8 9 10 11 A related question: what happens if we add two vectors of different lengths? 1:3 + 1:15 ## [1] 2 4 6 5 7 9 8 10 12 11 13 15 14 16 18 #is the same as rep(1:3, 5) + 1:15 ## [1] 2 4 6 5 7 9 8 10 12 11 13 15 14 16 18 This is recycling: R replicates the shorter vector to the same length as the longer vector and then adds the two together. While such usage is uncommon for other programming languages, it has undeniable utility, though one needs to be cautious when implementing this idea. 2.4.3 Note While recycling works for vectors, it doesnt do so for matrices or other rectangular data-types. 2.4.4 The c() operator The c() operator stands for concatenate or according to some writers, combine. y &lt;- 1:4 (y_2 &lt;- c(5:10, y)) #concatenate ## [1] 5 6 7 8 9 10 1 2 3 4 (y_3 &lt;- y_2[c(3:7)] )#note the square brackets ## [1] 7 8 9 10 1 (y_4 &lt;- y_2[-c(1, 4)]) #note the - sign ## [1] 6 7 9 10 1 2 3 4 2.4.5 Arrays and Matrices Matrices are two-dimensional data-types with columns and rows as the two dimensions. Arrays are data-types that can contain more than two dimensions as well (height, say in addition to rows and columns). These may or may not have special named attributes such as column names or row names. Matrices can be constructed from component vectors by the usage of commands cbind() and rbind() c_1 &lt;- c(4, 9, 10, 12) c_2 &lt;- c(10, 3, 1, 10) (mat_c &lt;- cbind(c_1, c_2)) #column-bind ## c_1 c_2 ## [1,] 4 10 ## [2,] 9 3 ## [3,] 10 1 ## [4,] 12 10 (mat_r &lt;- rbind(c_1, c_2)) #row-bind ## [,1] [,2] [,3] [,4] ## c_1 4 9 10 12 ## c_2 10 3 1 10 2.4.6 Dataframes Dataframes are data-types with a collection of vectors of the same length which may be of different types. Each column (variable) has a column-name that can be used to access the whole vector. Additionally, the column can be extracted by appending to the dataframe, the $ sign followed by the column-name. The full set of names can be extracted by the command names() df &lt;- data.frame(a = runif(1:10), b = rnorm(10, 0, 1), c = 11:20 ) df$a %&gt;% head() ## [1] 0.7962160 0.8256270 0.8806768 0.5027571 0.7302074 0.3600830 names(df) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; 2.4.7 Subsetting Dataframes If we need only part of a dataframe, we can refer to the relevant indices in square brackets appended to the name of the dataframe. Leaving an index blank includes all entries. df[, 1] %&gt;% head() #column 1 ## [1] 0.7962160 0.8256270 0.8806768 0.5027571 0.7302074 0.3600830 df[3, ] %&gt;% head() #row 3 ## a b c ## 3 0.8806768 -1.254443 13 df[c(1,2), &quot;c&quot;] ## [1] 11 12 Subsetting with Logical Indices df[df$b &lt; 0.5, ] #only those rows whose column b has values &lt; 0.5 ## a b c ## 1 0.7962160 -0.4358187 11 ## 3 0.8806768 -1.2544429 13 ## 6 0.3600830 -0.7157370 16 ## 7 0.9840496 -0.4980695 17 ## 10 0.1356141 -0.3081540 20 Subsetting and Assignment df[df$c &gt;= 14, &quot;a&quot;] &lt;- NA df ## a b c ## 1 0.7962160 -0.4358187 11 ## 2 0.8256270 0.7694979 12 ## 3 0.8806768 -1.2544429 13 ## 4 NA 1.0808217 14 ## 5 NA 1.2710286 15 ## 6 NA -0.7157370 16 ## 7 NA -0.4980695 17 ## 8 NA 3.1910734 18 ## 9 NA 0.6673823 19 ## 10 NA -0.3081540 20 2.5 Functions Functions automate common tasks succinctly. According to Hadley Wickham, a general rule of thumb is that if we need to copy-paste more than thrice, its time to write a function. Writing functions has many uses: its name betrays its intentions; if changes are needed only one edit is enough; and the chance of errors while copy-pasting decreases drastically when working with functions. Suppose there are normal random variables and we wish to transform them to standard normal. t_1 &lt;- rnorm(100, 5, 10) #number of points, mean, sigma t_2 &lt;- rnorm(100, 10, 100) t_3 &lt;- rnorm(100, 20, 200) t_1_s &lt;- (t_1-5)/10 t_2_s &lt;- (t_1-10)/100 t_3_s &lt;- (t_1-20)/200 However, this could be automated if we write a function norm_std &lt;- function(t, mu, sigma) #note the syntax { t_std &lt;- (t - mu)/sigma return(t_std) #note the return function } norm_std(t_1, 5, 10) %&gt;% head(.) #what does this mean? ## [1] 2.6600539 -0.7709417 -1.6094629 0.1946189 0.5406417 0.7344608 norm_std(t_2, 10, 100) %&gt;% head(.) ## [1] -0.6732682556 0.9066280680 2.4150492120 -3.3811156614 -0.0006845677 ## [6] 0.0406820311 norm_std(t_3, 20, 200) %&gt;% head(.) ## [1] 1.9888906 -0.4136522 0.4595195 0.8467714 0.2944065 1.4520311 2.6 Directory Management If we save something in R or if we wish to access some file in R, we need to ensure that they are in the same folder (directory) as our code. This gives rise to the notion of the working directory which is displayed at the top of the console. We can explicitly find the name of the working directory by the command getwd(). To pinpoint addresses of folders, Mac and Linux use slashes (say plots/plot_1.pdf) but Windows uses backslashes (plots\\plot_1.pdf). However, backslashes are reserved in Rin that they cannot be used as they are. Hence to include each backslash, we need to prefix another backslash, which may be confusing. Hence many writers suggest using the Linux/Mac style addresses. 2.7 RStudio Projects It is good practice to keep all files related to a project together in one folder. This may include script files (with .R extension), plots, data files etc. RStudio uses projects with .Rproj extension for this special purpose. In general, it is considered good practice to keep one project for each data analysis exercise and put all data files, scripts, outputs (such as plots) in that folder. It is also recommended that relative paths be used and not absolute paths. "],["introduction-to-financial-time-series.html", "Chapter 3 Introduction to financial time series 3.1 Background 3.2 Empirical Financial Time Series 3.3 Returns 3.4 Multi-period Simple Return 3.5 Computational Examples 3.6 Return Series for Market Indices 3.7 Stationarity of Time Series 3.8 Distribution of Returns 3.9 Stylized Facts 3.10 References", " Chapter 3 Introduction to financial time series 3.1 Background Financial prices, indices, returns etc. are sequences of real numbers indexed by time. The study of their mathematical and statistical properties is vital for those aspiring to write papers in empirical finance. For example, consider the following plot of a hypothetical market index: what can we say about the market conditions from its ups and downs? The above series is, however generated via the command: plot(cumsum(rnorm(10000, 0, 1)) which plots the cumulative sum of ten thousand standard normal realizationsa random walk! The random walk hypothesis is a theory that claims that stock market prices obey a random walk process. This hypothesis is consistent with the Efficient Market Hypothesis. Roughly speaking, the efficient market hypothesis claims in its weak form that for publicly traded stocks, all public information is captured in the price of the stock. The strong form of the hypothesis claims that all informationboth public and privateis reflected the stock price. Hence according to this theory, any attempt to consistently beat the market in terms of risk-adjusted returns is doomed to failure.1 Hence in particular, any attempt to discern patterns in the hypothetical markets ups and downs as shown above is worthless. Do empirical financial markets behave in the same way too? 3.2 Empirical Financial Time Series As an illustration we produce the daily time series for the closing value of the Bombay Stock Exchange index (Sensex). file_bse &lt;- &quot;SENSEX.csv&quot; index_bse &lt;- readr::read_csv(file_bse) %&gt;% dplyr::select(-empty) index_bse$Date &lt;- as.Date(index_bse$Date, format = &quot;%d-%B-%Y&quot; ) #date reformat plot(index_bse$Date, index_bse$Close, type = &quot;l&quot;, col = &quot;blue&quot;, xlab = &quot;Year&quot;, ylab = &quot;BSE Index&quot;, main = &quot;Indian stock market performance&quot; ) fit_lm &lt;- lm(Close ~ Date, data = index_bse) #fit linear model abline(fit_lm, #plot linear model line lty = &quot;dotdash&quot;, col = &quot;red&quot;, lwd = 2 ) # via ggplot ggplot(data = index_bse, aes(Date, Close) ) + geom_line(lwd = 0.3, color = &quot;blue&quot; ) + geom_smooth(method = &quot;lm&quot;, lty = &quot;dotdash&quot;, lwd = 0.6, color = &quot;red&quot;, se = F) + theme_minimal() + labs(x = &quot;Years&quot;, y = &quot;BSE Sensex&quot;, title = &quot;Indian stock market performance&quot; ) It seems that the level of the series is rising and the fluctuations are sometimes high and sometimes low. This index series is an example of a non-stationary time series. This roughly means that the mean and the variance of such a series are functions of time. Here is the index level time series for the US index S&amp;P500 from 2008: file_sp500 &lt;- &quot;SP500.csv&quot; #S&amp;P 500 ind_sp500 &lt;- readr::read_csv(file_sp500, col_types = cols(DATE = col_date(), SP500 = col_double() ), na = c(&quot;&quot;, &quot;NA&quot;, &quot;.&quot;) ) ggplot(ind_sp500, aes(DATE, SP500) ) + geom_line(lwd = 0.3, color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, lty = &quot;dotdash&quot;, lwd = 0.6, se = F, color = &quot;red&quot; ) + theme_minimal() + labs(x = &quot;Years&quot;, y = &quot;S&amp;P500&quot; ) ## Warning: Removed 92 rows containing non-finite values (stat_smooth). And here it for the Japanese Nikkei225: file_nikkei &lt;- &quot;NIKKEI225.csv&quot; #Nikkei 225 ind_nikkei &lt;- readr::read_csv(file_nikkei, col_types = cols(DATE = col_date(), NIKKEI225 = col_double() ), na = c(&quot;&quot;, &quot;NA&quot;, &quot;.&quot;) ) ggplot(ind_nikkei, aes(DATE, NIKKEI225) ) + geom_line(lwd = 0.3, color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, lty = &quot;dotdash&quot;, lwd = 0.6, se = F, color = &quot;red&quot; ) + theme_minimal() + labs(x = &quot;Years&quot;, y = &quot;Nikkei225&quot; ) 3.3 Returns We observe prices in the financial markets empirically. However, due to their non-stationary nature, they are hard to analyze. Hence they are converted to return series which are usually stationary. There are many ways to construct different notions of returns from the same underlying price sequence. We discuss some prominent ones below. 3.3.1 One-Period Simple Return The simple one period return for holding some asset whose price is given by the sequence \\(\\{p_t\\}_{t=1}^n\\) is: \\[r_t := \\frac{p_t-p_{t-1}}{p_{t-1}} = \\frac{p_t}{p_{t-1}}-1\\] 3.4 Multi-period Simple Return \\[r_t[k] := \\frac{p_t-p_{t-k}}{p_{t-k}} = \\frac{p_t}{p_{t-k}}-1\\] \\[r_t[k] := \\frac{p_t}{p_{t-k}}-1 = \\frac{p_t}{p_{t-1}}\\frac{p_{t-1}}{p_{t-2}}\\hdots\\frac{p_{t-k+1}}{p_{t-k}}-1\\] \\[r_t[k] := (1+r_t)(1+r_{t-1})\\hdots(1+r_{t-k+1})-1\\] Multi-period returns are used to convert high frequency returns to low frequency returnsi.e., daily to monthly; or monthly to yearly etc. For example to convert monthly returns to annual returns: \\[r_{t}[12]= [\\prod_{j=0}^{12-1}(1+r_{t-j})]^{1/12}-1\\] Often (when working with daily returns especially) \\(1+r_t\\approx r_t\\), in which case: \\[r_t[k]\\approx \\sum_{j=0}^{k-1}r_{t-j}\\] Additionally, the simple returns for a portfolio with fractional weights \\(w_1,\\hdots,w_n\\) are: \\[r_{p,t}=\\sum_{i=1}^n w_ir_{i,t}\\] 3.4.1 Log Returns We know that to compute yearly returns from say, monthly returns we use the following formula: \\[r_{t}[12]= [\\prod_{j=0}^{12-1}(1+r_{t-j})]^{1/12}-1\\] In general, if a bank pays an annual interest of \\(r^m_t\\) \\(m\\) times a year, the interest rate for unit investment is \\(r^m_t/m\\) and after one year the value of the deposit is \\((1+\\frac{r_t^m}{m})^m\\). If there is continous compounding, \\(m\\to \\infty\\), in which case the value of the investment becomes: \\[\\lim\\limits_{m\\to \\infty}(1+\\frac{r_t^m}{m})^m = e^{r_t}\\] Hence it must be that: \\[R_t = \\log p_t-\\log p_{t-1} = \\log\\frac{p_t}{p_{t-1}}\\] A particular advantage of log-returns are that multiperiod log returns are merely the sum of one period log returns: \\[R_t[k]=\\log p_t - \\log p_{t-k}=\\log p_t - \\log p_{t-1} + \\log p_{t-1} -\\hdots \\log p_{t-k}\\] \\[R_t[k]= R_t[1]+R_t[2]+\\hdots+R_t[k-1]\\] 3.5 Computational Examples To construct return series from prices, we write a function that accepts as input a price (or index) sequence and returns a sequence of simple one period returns by using the formula \\(r_t = \\frac{p_t-p_{t-1}}{p_{t-1}}\\). Since there is no return for the first entry of the price sequence, we append an NA at the beginning of the returns. func_pr_to_ret &lt;- function(price_vec) { # This function takes a vector of prices and # returns a vector of simple one period returns l &lt;- length(price_vec) ret_num &lt;- diff(price_vec) #numerator = change in prices ret_den &lt;- price_vec[-l] #denominator = price series return(c(NA, ret_num/ret_den)) #first return is NA } price_vec_1 &lt;- seq(from = 1, to = 10, by = 2) func_pr_to_ret(price_vec_1) ## [1] NA 2.0000000 0.6666667 0.4000000 0.2857143 Similarly log-returns can be calculated using the formuala \\(R_t = \\log(p_t)-\\log(p_{t-1})\\). func_pr_to_logret &lt;- function(p_vec) { # This function takes a vector of prices and # returns a vector of log returns log_price &lt;- log(p_vec) log_ret &lt;- diff(log_price) #log ret = delta log p return(c(NA, log_ret)) #first return is NA } p_vec &lt;- seq(from = 1, to = 10, by = 2) func_pr_to_logret(p_vec) ## [1] NA 1.0986123 0.5108256 0.3364722 0.2513144 3.6 Return Series for Market Indices We can compute returns and log-returns for the Bombay Stock Exchange Index, S&amp;P500 and the Nikkei 225 as follows: ret_BSE &lt;- func_pr_to_ret(index_bse$Close) plot(index_bse$Date, #x variable ret_BSE, #y variable type = &quot;l&quot;, lwd = 1.2, col = &quot;blue&quot;, xlab = &quot;Years&quot;, ylab = &quot;BSE Returns&quot; ) abline(h = 0) We can do the same for the S&amp;P500 and the Nikkei225 series: ret_sp500 &lt;- func_pr_to_ret(ind_sp500$SP500) plot(ind_sp500$DATE, #x variable ret_sp500, #y variable type = &quot;l&quot;, lwd = 1.2, col = &quot;red&quot;, xlab = &quot;Years&quot;, ylab = &quot;S&amp;P500 Returns&quot; ) abline(h = 0) ret_nikkei &lt;- func_pr_to_ret(ind_nikkei$NIKKEI225) plot(ind_nikkei$DATE, #x variable ret_nikkei, #y variable type = &quot;l&quot;, lwd = 1.2, col = &quot;green&quot;, xlab = &quot;Years&quot;, ylab = &quot;Nikkei225 Returns&quot; ) abline(h = 0) One clearly sees that the index sequences have no trend now. It seems that the returns are fluctuating around a mean level 0. The fluctuations (volatility) around the mean are sometimes high and sometimes low. 3.7 Stationarity of Time Series Theres a clear difference between the plots of prices (or indices like the BSE); and the returns derived from them. Indices keep increasing with time, suggesting some type of a trend and tend to fluctuate more or less around the trend line. Returns, on the other hand seem to have no trend. 3.7.1 Stationarity A (strictly) stationary process is one for which the unconditional joint distribution does not change with time: \\[F_X(x_1,\\hdots,x_t)=F_X(x_{1+\\delta},\\hdots,x_{t+\\delta})\\] where \\(F_X(\\cdot)\\) is the distribution for the process \\(X_t\\) and \\(\\delta&gt;0\\). In particular this implies that the mean and the variance (and other central moments) do not change with time. Inference for nonstationary series is difficult. Hence in practice we convert nonstationary processes to stationary processes before analyzing them. A weakly stationary time series is such that the mean of the process \\(\\mathbb{E}(X_t) = \\mu(t) = \\mu\\) and its (auto)covariance \\(\\text{covar}(X_t, X_{t-\\delta}) = \\sigma_{\\delta}\\) depends only on the lag \\(\\delta\\). Strictly stationary series are weakly stationary but not vice versa. 3.7.2 Trend Stationarity If the trend (mean) of the nonstationary process is deterministic, we have trend stationarity: \\[X_t = \\mu(t) + \\epsilon_t\\] where \\(\\mu(\\cdot)\\) is a real function and \\(\\epsilon_t\\) is stationary process (say, \\(\\mathcal{N}(0,1)\\)). Here is an illustrationsuppose the mean is \\(\\mu(t) = 1 + 2t\\): t &lt;- seq(0, 100, 0.4) plot((1 + 2*t + rnorm(length(t), 0, 10)), type = &quot;l&quot;, lwd = 2, col = &quot;green&quot;, xlab = &quot;index&quot;, main = &quot;Trend stationary seris&quot; ) lines((1+2*t), type = &quot;l&quot;, col = &quot;red&quot;, lty = &quot;dotdash&quot;, lwd = 1.2 ) This can be made into a stationary process by detrending: subtracting \\(\\mu(t) = 1+2t\\) from the series, to recover \\(X_t-\\mu(t)=\\epsilon_t\\sim \\mathcal{N}(0, 10)\\) in this case: plot((1 + 2*t + rnorm(length(t), 0, 10) - (1 + 2*t)), type = &quot;l&quot;, main = &quot;Detrended series&quot;, col = &quot;green&quot; ) abline(h = 0) Additionally trend stationary series are mean-reverting which means that after encountering an exogenous shock they tend to revert to the fluctuations-around-the-mean behavior. 3.7.3 Unit Roots A stochastic process has a unit root if 1 is the root of its characteristic equation. For example consider the following process: \\[X_t = X_{t-1} + \\epsilon_t\\] Its characteristic equation is: \\(x - 1 = 0\\). Its clear that \\(X_t - X_{t-1} = \\epsilon_t\\) is a stationary process. Note that for trend stationary series we detrended to obtain a stationary series while for a unit root process, we first-differenced to arrive at a stationary series. Random walks are examples of unit root processes. By repeated additions: \\[X_T = X_0 + \\sum_{t = 1}^T \\epsilon_t\\] Hence \\(\\mathbb{E}(X_T) = X_0 + \\mathbb{E}(\\epsilon_T) = X_0\\); and \\(\\text{var}(X_T) = \\text{var}(\\sum_{t = 1}^T \\epsilon_t) = T\\cdot \\sigma^2\\). plot(cumsum(rnorm(1000, 0, 10)), type = &quot;l&quot;, lwd = 1.5, col = &quot;red&quot;, main = &quot;Unit root process&quot; ) plot(rnorm(1000, 0, 10), type = &quot;l&quot;, col = &quot;blue&quot;, main = &quot;Differenced unit root process&quot; ) Unit root processes are not mean revertingan exogenous shock can permanently alter the behavior of the unit root process. 3.8 Distribution of Returns Are all return values for the indices considered above equally likely to occur? If that is the case, the returns will be uniformly distributed. A quick glance at the figures however suggests that perhaps some other distribution ought to be considered. 3.8.1 Summary Statistics of Empirical Returns For the three indices and their return series considered above (the BSE Sensex, the S&amp;P500, the Nikkei225) we compute summary statistics below: func_summ_stat_ind &lt;- function(vec) { summ_stat_ind &lt;- data.frame(min = min(vec, na.rm = T), max = max(vec, na.rm = T), mean = mean(vec, na.rm = T), median = median(vec, na.rm =T), std = sd(vec, na.rm =T), iqr = IQR(vec, na.rm=T), var = var(vec, na.rm =T) ) %&gt;% signif(., digits = 2) #significant digits = 2 return(summ_stat_ind) } table_summ_stat &lt;- rbind(c(&quot;BSE&quot;, func_summ_stat_ind(ret_BSE)), c(&quot;S&amp;P500&quot;, func_summ_stat_ind(ret_sp500)), c(&quot;Nikkei225&quot;, func_summ_stat_ind(ret_nikkei)) ) knitr::kable(table_summ_stat) min max mean median std iqr var BSE -0.11 0.17 0.00061 0.00092 0.014 0.014 2e-04 S&amp;P500 -0.09 0.12 0.00038 0.00061 0.013 0.0093 0.00016 Nikkei225 -0.11 0.1 0.00014 0.00059 0.016 0.015 0.00024 For all series, the medians are about 1.53 times the corresponding means. This suggests that there are significant negative outliers. In general though, a natural choice for modeling return distribution is the ubiquitous normal distribution. To check if it is a plausible candidate, we plot the empirical histogram of returns for all three series separately. 3.8.2 Histograms for returns for each index hist(ret_BSE, breaks = 80, col = &quot;grey&quot;, xlab = &quot;Daily Returns&quot;, ylab = &quot;Frequency&quot;, main = &quot;Histogram for BSE&quot; ) hist(ret_sp500, breaks = 80, col = &quot;grey&quot;, xlab = &quot;Daily Returns&quot;, ylab = &quot;Frequency&quot;, main = &quot;Histogram for S&amp;P500&quot; ) hist(ret_nikkei, breaks = 80, col = &quot;grey&quot;, xlab = &quot;Daily Returns&quot;, ylab = &quot;Frequency&quot;, main = &quot;Histogram for Nikkei225&quot; ) The three histograms do resemble a normal density. All of them display the classic bell curve characteristics. However, merely eyeballing the data is not proof enough. 3.8.3 Skewness and Kurtosis One way to test if the data are normally distributed is to check their third and fourth central momentsthe skewness and kurtosis. For normal random variables, the skewness must be 0 (the bell curve is unimodal and symmetric about its mean); and the kurtosis must be 3. If the data display substantial departures from such values there may be evidence of non-normality. Formally the skewness and kurtosis are defined as: \\[s = \\mathbb{E}(\\frac{X-\\mu}{\\sigma})^3\\] \\[\\kappa = \\mathbb{E}(\\frac{X-\\mu}{\\sigma})^4\\] We compute the skewness and kurtosis of the return series for the three indices as follows: skew &lt;- rbind(moments::skewness(ret_BSE, na.rm = T), moments::skewness(ret_sp500, na.rm = T), moments::skewness(ret_nikkei, na.rm = T) ) kurt &lt;- rbind(moments::kurtosis(ret_BSE, na.rm = T), moments::kurtosis(ret_sp500, na.rm = T), moments::kurtosis(ret_nikkei, na.rm = T) ) table_skew_kurt &lt;- data.frame(Skew = skew, Kurt = kurt, Excess_Kurt = kurt-3 ) knitr::kable(table_skew_kurt) Skew Kurt Excess_Kurt 0.0967129 12.870632 9.870632 -0.0938530 15.121463 12.121463 -0.6276185 9.691047 6.691047 There is substantial evidence from the table above to suggest that the returns may be non-normal. Formally however, we resort the Jarque-Bera Test to confirm our hypothesis. 3.8.4 The Jarque-Bera Test for Normality There is a large literature that suggests two empirical regularities for asset returns: Crashes occur more frequently than booms Extreme events occur more frequently than suggested by normal distributions The first outcome is consistent with a distribution displaying negative skewness. The second outcome suggests excess kurtosis. The Jarcque-Bera test is a moment based test that relis on the observation that for a normal random variable the skewness and excess kurtosis are both 0. The estimators for skewness and kurtosis are: \\[\\hat{s} = \\frac{1}{T}\\sum_{t=1}^T (\\frac{x_t-\\bar{x}}{\\hat{\\sigma}})^3\\] \\[\\hat{\\kappa} = \\frac{1}{T}\\sum_{t=1}^T (\\frac{x_t-\\bar{x}}{\\hat{\\sigma}})^4\\] and as \\(T\\to \\infty\\) \\[\\sqrt T\\cdot \\hat{s} \\to \\mathcal{N}(0, 6)\\] \\[\\sqrt T\\cdot (\\hat{\\kappa}-3) \\to \\mathcal{N}(0, 24)\\] Hence, for normal random variables \\[\\frac{\\hat{s}}{\\sqrt(\\frac{6}{T})}\\to \\mathcal{N}(0,1)\\] \\[\\frac{\\hat{\\kappa}-3}{\\sqrt(\\frac{24}{T})}\\to \\mathcal{N}(0,1)\\] The Jarque-Bera test statistic uses the following insight: \\[JB = T[\\frac{\\hat{s}^2}{6}+\\frac{\\hat{\\kappa}-3}{24}]\\to \\chi^2(2)\\] In order to test if the returns from the three indices do follow the normal distribution we employ the package tseries and the function jarque.bera.test() in it. Note that it disallows any NA values. tseries::jarque.bera.test(rnorm(1000, 0, 10)) #benchmarking ## ## Jarque Bera Test ## ## data: rnorm(1000, 0, 10) ## X-squared = 0.71894, df = 2, p-value = 0.698 tseries::jarque.bera.test(!is.na(ret_BSE)) #note: !is.na() ## ## Jarque Bera Test ## ## data: !is.na(ret_BSE) ## X-squared = 3465305843, df = 2, p-value &lt; 2.2e-16 tseries::jarque.bera.test(!is.na(ret_sp500)) ## ## Jarque Bera Test ## ## data: !is.na(ret_sp500) ## X-squared = 14394, df = 2, p-value &lt; 2.2e-16 tseries::jarque.bera.test(!is.na(ret_nikkei)) ## ## Jarque Bera Test ## ## data: !is.na(ret_nikkei) ## X-squared = 5265.2, df = 2, p-value &lt; 2.2e-16 3.9 Stylized Facts Stock prices, commodity prices, exchange rates etc. in empirical financial markets display many striking regularities discussed in (Cont:2001?). Fat Tails: Unconditional return distributions have tails fatter than those of normal distribution. Conditional return distributions are also non-normal. Asymmetry: Unconditional return distributions are negatively skewed. Aggregated Normality: Lower frequency returns resemble normal distributions more than higher frequency returns. No Autocorrelation: Except at high frequencies, returns generally do not display autocorrelation. Volatility Clustering: Return volatility is autocorrelated. Time-Varying Cross Correlation: Correlation between assets returns tends to be higher during high volatility periods especially during market crashes. 3.10 References Or, in other words, if investors beat the market its not perhaps due to their so-called investing skills but due to their being lucky. "],["introduction-to-arma-garch-processes.html", "Chapter 4 Introduction to ARMA-GARCH processes 4.1 The Autocorrelation Function (ACF) 4.2 Autoregressive (AR) Processes 4.3 Moving Average (MA) Processes 4.4 Autoregressive Moving Average (ARMA) Processes 4.5 Conditional Heteroskedasticity Models 4.6 Autoregressive Conditional Heteroskedasticity (ARCH) 4.7 Generalized ARCH (GARCH) Processes 4.8 Application: Fitting ARMA GARCH Models 4.9 References", " Chapter 4 Introduction to ARMA-GARCH processes 4.1 The Autocorrelation Function (ACF) The correlation between random variables \\(X_1, X_2\\) is a measure of their linear dependence and is defined as: \\[\\rho_{12}:= \\frac{\\text{cov}(X_1,X_2)}{\\sqrt{\\text{var}(X_1)\\text{var}(X_2)}}=\\frac{\\sigma_{12}}{\\sigma_1\\sigma_2}\\] It lies between -1 and 1 and for normal random variables \\(\\rho_{12}=0\\) implies that the variables are independent. If we have a sample \\(\\{x_{1,t}, x_{2,t}\\}_{t=1}^T\\) the correlation can be consistently estimated by computing sample correlation: \\[\\hat{\\rho}_{12}=\\frac{\\hat{\\sigma}_{12}}{\\hat{\\sigma}_1\\hat{\\sigma}_2}\\] For a time series \\(r_t\\) which is weakly stationary, the lag-\\(l\\) autocorrelation function is the correlation between \\(r_t\\) and \\(r_{t-l}\\): \\[\\rho_l=\\frac{\\sigma_{t,t-l}}{\\sigma_t\\sigma_{t-l}}=\\frac{\\sigma_{t,t-l}}{\\sigma_t^2} =\\frac{\\gamma_l}{\\gamma_0}\\] This follows from weak stationarity: \\(\\sigma^2_t=\\sigma^2_{t-l}=\\gamma_0\\) and \\(\\text{cov}(r_t,r_{t-l})=\\gamma_l\\). We claim that there is no autocorrelation if \\(\\rho_l=0\\) \\(\\forall l&gt;0\\). To estimate the autocorrelation function of lag (say) 1, we use its sample counterpart: \\[\\hat{\\rho}_1=\\frac{\\sum_{t=2}^T (r_t-\\bar{r})(r_{t-1}-\\bar{r})}{\\sum_{t=1}^T (r_t-\\bar{r})^2}\\] In general for lag \\(l\\) we consistently estimate it as: \\[\\hat{\\rho}_l=\\frac{\\sum_{t=l+1}^T (r_t-\\bar{r})(r_{t-1}-\\bar{r})}{\\sum_{t=1}^T (r_t-\\bar{r})^2}\\] The statistic \\(\\hat{\\rho_1},\\hat{\\rho_2},\\hdots\\) is the sample autocorrelation function of \\(r_t\\) and is key to capturing the linear dependence nature of the time series in question. 4.2 Autoregressive (AR) Processes Perhaps last periods returns may have some significant impact on the value of the returns this period. If so, its lag-1 autocorrelation may be useful for predicting the current periods value: \\[r_t = \\phi_0+\\phi_1r_{t-1}+u_t\\] where \\(u_t\\) is weakly stationary with mean 0 and variance \\(\\sigma^2_u\\). This is simply equivalent to a regression where \\(r_{t-1}\\) is the explanatory or independent variable. Its straightforward to check the conditional mean and variance of such a process: \\[\\mathbb{E}(r_t|r_{t-1}) = \\phi_0+\\phi_1r_{t-1}\\] \\[\\text{var}(r_t|r_{t-1}) = \\sigma_u^2\\] And more generally there could be defined autoregressive processes of order \\(p\\) (\\(AR(p)\\)): \\[r_t=\\phi_0+\\phi_1r_{t-1}+\\hdots+\\phi_pr_{t-p} + u_t\\] 4.2.1 AR(1) processes Is the AR(1) process \\(r_t=\\phi_0+\\phi_1r_{t-1}+u_t\\) weakly stationary? This will imply that its unconditional mean and variance must be fixed in time and lag-\\(l\\) covariance must depend only on the lag length \\(l\\). \\[\\mathbb{E}(r_t) = \\phi_0 + \\phi_1\\mathbb{E}(r_{t-1})+\\mathbb{E}(u_t)\\] \\[\\mathbb{E}(r_t) = \\phi_0 + \\phi_1\\mu\\] \\[\\mu = \\frac{\\phi_0}{1-\\phi_1}\\] This clearly implies that for the mean of an AR(1) process to exist, \\(\\phi_1\\neq 1\\) and \\(\\phi_0=\\mu\\cdot(1-\\phi_1)=\\mu-\\mu\\phi_1\\). Hence a weakly stationary AR(1) process is: \\[r_t=\\mu-\\mu\\phi_1+\\phi_1r_{t-1}+u_t\\] \\[r_t-\\mu=(r_{t-1}-\\mu)\\phi_1+u_t\\] \\[r_t-\\mu=((r_{t-2}-\\mu)\\phi_2+u_{t-1})\\phi_1+u_t\\] \\[\\vdots\\] \\[r_t-\\mu = u_t + \\phi_1u_{t-1}+\\phi_1^2u_{t-2}+\\hdots\\] \\[r_t=\\mu+\\sum_{i=0}^\\infty\\phi_1^i\\cdot u_{t-i}\\] Additionally, \\[\\text{var}(r_t)=\\phi_1^2\\text{var}(r_{t-1})+\\sigma^2_u\\] Since for weakly stationary AR(1) processes \\(\\text{var}(r_t)=\\text{var}(r_{t-1})=\\gamma_0\\) we have \\[\\gamma_0 = \\frac{\\sigma_u^2}{1-\\phi_1^2}\\] Weak stationarity immediately implies that \\(\\phi_1\\in(-1,1)\\). Hence taken together, for an AR(1) process to be weakly stationary it is necessary and sufficient that \\(\\phi_1\\in(-1,1)\\);2 and the canonical AR(1) series can be written as: \\[r_t=(1-\\phi_1)\\mu+\\phi_1r_{t-1}+u_t\\] We can plot some hypothetical autoregressive processes by simulation via the function arima.sim() included in the stats package that loads by default. # AR(0) plot(rnorm(500, 0, 0.8), type = &quot;l&quot;, col = &quot;blue&quot;) abline(h = 0) # AR(1) ar_1 &lt;- arima.sim(n = 500, list(ar = c(0.8)), sd = 0.8) plot(ar_1, col = &quot;blue&quot;) abline(h = 0) # AR(2) ar_2 &lt;- arima.sim(n = 500, list(ar = c(0.8, 0.15)), sd = 0.8) plot(ar_2, col = &quot;blue&quot;) abline(h = 0) # AR(3) ar_3 &lt;- arima.sim(n = 500, list(ar = c(0.5, 0.3, 0.15)), sd = 0.8) plot(ar_3, col = &quot;blue&quot;) abline(h = 0) 4.2.2 Autocorrelation Function for AR(1) processes We can easily check that for positive lags \\(l&gt;0\\), the lagged covariance follows: \\[\\gamma_l = \\phi_1\\gamma_{l-1}\\] Hence it follows that for the autocorrelation function \\(\\rho_l = \\phi_1\\rho_{l-1}\\); and because \\(\\rho_0=1\\), \\(\\rho_l = \\phi_1^l\\). This implies that the autocorrelation function of an AR(1) series decays exponentially with rate \\(\\phi_1\\) and starting value 1. If \\(\\phi_1&lt;0\\) the series alternates between positive and negative terms. Illustration For example lets compute the sample autocorrelation function (ACF) for the financial market indices. acf(ret_BSE, na.action = na.pass) acf(ret_sp500, na.action = na.pass) acf(ret_nikkei, na.action = na.pass) What about log-returns? logret_BSE &lt;- func_pr_to_logret(index_bse$Close) logret_SP &lt;- func_pr_to_logret(ind_sp500$SP500) logret_Nikkei &lt;- func_pr_to_logret(ind_nikkei$NIKKEI225) ACF_BSE &lt;- acf(logret_BSE, na.action = na.pass) #barplot(head(ACF_BSE$acf)) ACF_SP &lt;- acf(logret_SP, na.action = na.pass) #barplot(head(ACF_SP$acf)) ACF_Nikkei &lt;- acf(logret_Nikkei, na.action = na.pass) #barplot(head(ACF_Nikkei$acf)) 4.2.3 Partial Autocorrelation Functions (PACF) Is there a way to know how many lags to include for an autoregressive return series? This issue is solved via the usage of partial autocorrelation functions as shown below. Consider the following sequences of AR processes: \\[r_t=\\phi_{01}+ \\phi_{11}r_{t-1}+u_{1t}\\] \\[r_t=\\phi_{02}+ \\phi_{12}r_{t-1}+\\phi_{22}r_{t-2}+u_{2t}\\] \\[r_t=\\phi_{03}+ \\phi_{13}r_{t-1}+\\phi_{23}r_{t-2}+\\phi_{33}r_{t-3}+u_{3t}\\] \\[r_t=\\phi_{04}+ \\phi_{14}r_{t-1}+\\phi_{24}r_{t-2}+\\phi_{34}r_{t-3}+\\phi_{44}r_{t-4}+u_{4t}\\] \\[\\vdots\\] These models are merely multiple regressions and can be estimated via the standard least squares method. In these models, \\(\\hat{\\phi}_{11}\\) is called the lag 1 sample PACF of \\(r_t\\), \\(\\hat{\\phi}_{22}\\) of the second equation is the lag 2 sample PACF of \\(r_t\\) and so on. By construction, the lag 2 \\(\\hat{\\phi}_{22}\\) is the marginal contribution of \\(r_{t-2}\\) in explaining \\(r_t\\) over the AR(1) model and so on. Hence if the underlying model is say AR(\\(p\\)) then all sample PACFs \\(\\hat{\\phi}_{11},\\hdots, \\hat{\\phi}_{pp}\\) must be different from 0 but all sample PACFs from then on: \\(\\hat{\\phi}_{p+1,p+1}, \\hdots=0\\). This property can be used to find the order \\(p\\). Armed with this knowledge, lets compute the PACFs for the three financial market indices: pacf(ret_BSE, na.action = na.pass) pacf(ret_sp500, na.action = na.pass) pacf(ret_nikkei, na.action = na.pass) 4.2.4 Information Criteria Apart from PACF, another way to find the number of lags is the use of likelihood based information criteria. Here we look at the two most famous ones: the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). \\[\\text{AIC}(l) = -\\frac{2}{T}\\cdot\\ln(\\text{likelihood})+\\frac{2}{T}\\cdot(\\#\\text{parameters})\\] For a Gaussian AR(\\(l\\)), \\(AIC=\\ln(\\hat{\\sigma}^2_{u,MLE})+2\\frac{l}{T}\\). The first term measures the goodness of fit of the model while the second penalizes the usage of parameters. The Bayesian Information Criterion (BIC) uses a different penalty function. For a Gaussian AR(\\(l\\)) is takes the following form: \\[BIC(l)=\\ln(\\hat{\\sigma}^2_{u,MLE})+\\ln(T)\\cdot \\frac{l}{T}\\] 4.3 Moving Average (MA) Processes Consider an infinitely long autoregressive process: \\[r_t = \\phi_0+\\phi_1r_{t-1}+\\phi_2r_{r-2}+\\hdots+u_t\\] If this series is to be weakly stationary, the coefficients \\(\\phi_j\\) must decay sufficiently fast. One way to ensure this is to assume that \\(\\phi_j=-\\theta^j\\) for some \\(\\theta\\in(0,1)\\). \\[r_t = \\phi_0-\\theta_1r_{t-1}-\\theta_1^2r_{t-2}-\\hdots+u_t\\] \\[r_t+\\sum_{j=1}^\\infty \\theta_1^jr_{t-j}=\\phi_0+u_t\\] The same form can be written for \\(r_{t-2}\\): \\[r_{t-1}+\\sum_{j=2}^\\infty \\theta_1^jr_{t-j}=\\phi_0+u_{t-1}\\] Solving for the above two equations, we get: \\[r_t = \\phi_0(1-\\theta_1) + u_t -\\theta_1u_{t-1}\\] This indicates the AR model is a weighted average of shocks \\(u_t, u_{t-1}\\) and a constant. This is a moving average form of order 1 or MA(1). Its straightforward to check that unlike AR processes, MA processes are always stationary. (Can you see why?) The general form for the MA(\\(q\\)) process is: \\[r_t = c_0 + u_t -\\theta_1u_{t-1}-\\theta_2u_{t-2}-\\hdots-\\theta_qu_{t-q}\\] 4.3.1 Autocorrelation Function Consider the MA(1) model with the constant term 0: \\[r_{t} = u_t -\\theta_1u_{t-1}\\] \\[r_{t-l}r_t = u_tr_{t-l} -\\theta_1u_{t-1}r_{t-l}\\] \\[\\mathbb{E}(r_{t-l}r_t)=0-\\theta_1\\mathbb{E}(u_{t-1}r_{t-l})\\] From this we see that: \\[\\gamma_1=-\\theta_1\\sigma^2_u\\] \\[\\gamma_{l&gt;1}=0\\] Also, \\(\\text{var}(r_t) =\\gamma_0= (1+\\theta_1^2)\\sigma_u^2\\) and this implies that for \\(\\rho_l = \\frac{\\gamma_l}{\\gamma_0}\\) becomes: \\[\\rho_0 = 1\\] \\[\\rho_1 = -\\frac{\\theta_1}{1+\\theta_1^2}\\] \\[\\rho_{l&gt;1} = 0\\] Hence for MA(1) processes, while the first lag autocorrelation is nonzero, all further lags produce zero autocorrelation. This property can be exploited to locate the order of the MA process. In general, for an MA(\\(q\\)) process, the ACF cuts off at lag \\(q\\). Since the MA(\\(q\\)) process only relies on its past \\(q-1\\) realization, its often called a finite memory process. To check the order of the MA series in question, we plot the ACF of the series. If \\(\\rho_q\\neq 0\\) but \\(\\rho_{l&gt;q}=0\\) we may conclude that the series is MA(\\(q\\)). 4.3.2 Illustrations We simulate some moving average processes below: n &lt;- 500 # MA(0) plot(rnorm(n), type = &quot;l&quot;, col = &quot;blue&quot;) abline(h = 0) # MA(1) ma_1 &lt;- arima.sim(n = n, list(ma = c(0.8)), innov=rnorm(n)) plot(ma_1, col = &quot;blue&quot;) abline(h = 0) acf(ma_1) # MA(2) ma_2 &lt;- arima.sim(n = n, list(ma = c(0.8, 0.15)), innov=rnorm(n)) plot(ma_2, col = &quot;blue&quot;) abline(h = 0) acf(ma_2) # MA(3) ma_3 &lt;- arima.sim(n = n, list(ma = c(0.5, 0.3, 0.15)), innov=rnorm(n)) plot(ma_3, col = &quot;blue&quot;) abline(h = 0) acf(ma_3) 4.4 Autoregressive Moving Average (ARMA) Processes While in principle we could fit empirical time series to AR(\\(p\\)) or MA(\\(q\\)) models exclusively, often the concomitant order is very high. In order to circumevent this problem, we combine AR and MA models into a composite ARMA(\\(p,q\\)) model with fewer parameters to estimate. For example the ARMA(1,1) series is as follows: \\[r_t-\\phi_1r_{t-1} = \\phi_0 + u_t -\\theta_1u_{t-1}\\] The LHS is the AR component while the RHS is MA component. 4.4.1 Properties Taking expectations we get: \\[\\mathbb{E}(r_t)-\\phi_1\\mathbb{E}(r_{t-1}) = \\phi_0 + \\mathbb{E}(u_t) - \\theta_1\\mathbb{E}(u_{t-1})\\] \\[\\mathbb{E}(r_t) = \\mu = \\frac{\\phi_0}{1-\\phi_1}\\] This is the same exact mean as that of an AR(1) process. Solving for the stationarity of the variance we get the same condition for the parameter \\(\\phi_1\\in (0,1)\\) as that of the AR(1) model. It is not hard to derive the autocorrelation function of the ARMA(1,1) process which behaves the same way as that of an AR(1) process. In general for an ARMA(\\(p, q\\)) process we have the following definition: \\[r_t = \\phi_0 + \\sum_{i=1}^p \\phi_ir_{t-i}+u_t-\\sum_{j=1}^q \\theta_ju_{t-j}\\] Here are some simulations from ARMA models. n_arma &lt;- 300 #ARMA(1,1) arma_11 &lt;- arima.sim(n = n_arma, list(ar = c(0.5), ma = c(0.5)), innov = rnorm(n_arma) ) plot(arma_11, col = &quot;blue&quot;) abline(h = 0 ) #ARMA(2,2) arma_22 &lt;- arima.sim(n = n_arma, list(ar = c(0.5, 0.4), ma = c(0.5, 0.4)), innov = rnorm(n_arma) ) plot(arma_22, col = &quot;blue&quot;) abline(h = 0 ) 4.5 Conditional Heteroskedasticity Models In empirical time series, we do not observe volatility directly but only estimate it with the help of some other directly observed characteristics. However as discussed in (Cont:2001?) volatility displays some striking regularities such as volatility clustering and differential reaction to price changes. To investigate the volatility of empirical time series, we plot the daily returns and ACF of the Bombay stock exchange sensex. plot(logret_BSE, type = &quot;l&quot;, col = &quot;blue&quot;) abline(h = 0) plot(acf(ret_BSE, na.action = na.pass)) While there seems to be no serial autocorrelation there is some dependence structure embedded in the series. We can check this for various functions of the return process such as squared returns or absolute returns: ret_BSE_sq &lt;- ret_BSE^2 acf(ret_BSE_sq, na.action = na.pass) pacf(ret_BSE_sq, na.action = na.pass) ret_BSE_abs &lt;- abs(ret_BSE) acf(ret_BSE_abs, na.action = na.pass) pacf(ret_BSE_abs, na.action = na.pass) This behavior is also displayed by the other financial markets. acf(ret_sp500^2, na.action = na.pass) pacf(ret_sp500^2, na.action = na.pass) acf(ret_nikkei^2, na.action = na.pass) pacf(ret_nikkei^2, na.action = na.pass) The conditional mean volatility of the series may be written as: \\[\\mu_t = \\mathbb{E}(r_t|\\mathbb{F}_{t-1})\\] \\[\\sigma_t^2 = \\text{var}(r_t|\\mathbb{F}_{t-1}) = \\mathbb{E}((r_t-\\mu_t)^2|\\mathbb{F}_{t-1}) =\\text{var}(u_t|\\mathbb{F}_{t-1})\\] Here \\(\\mathbb{F}(\\cdot)\\) denotes the filtration or the information available at the given time. 4.5.1 ARCH Effect Roughly speaking if the squared residuals display autocorrelation there exist ARCH effects (autoregressive conditional heteroskedasticity effects). Consider \\(r_t = \\mu_t + u_t\\). The square of the residual series \\(u_t = r_t - \\mu_t\\) is of interest for estimating conditional heteroskedasticity. A popular method of testing for ARCH effects is via the Ljung and Box statistics applied to the squared residuals series. To test if autocorrelations of \\(\\{r_t\\}\\) are jointly 0, the Portmanteau statistic (also called the Box-Pierce statistic) is used: \\[Q^*_m = T\\cdot\\sum_{l=1}^m \\hat{\\rho}^2_l\\] The null hypothesis is \\(H_0:\\rho_1=\\rho_2=\\hdots=\\rho_m=0\\) and the alternative is that for some \\(i\\), \\(\\rho_i\\neq 0\\). If \\(\\{r_t\\}_{t=1}^T\\) are iid with some moment conditions, \\(Q^*(m)\\sim\\chi^2(m)\\). The Ljung-Box test statistic is based on the Portmanteau statistic but increases its power via the following: \\[Q(m)=T(T+2)\\cdot\\sum_{l=1}^m \\frac{\\hat{\\rho}_l^2}{T-l}\\] We test for ARCH effects via the function Box.test() including in the stats() package Box.test(rnorm(100)^2, type = &quot;Ljung-Box&quot;) #Benchmark squared normals ## ## Box-Ljung test ## ## data: rnorm(100)^2 ## X-squared = 1.3782, df = 1, p-value = 0.2404 Box.test(ret_BSE_sq, type = &quot;Box-Pierce&quot;) #Squared BSE returns ## ## Box-Pierce test ## ## data: ret_BSE_sq ## X-squared = 152.37, df = 1, p-value &lt; 2.2e-16 Box.test(ret_BSE_abs, type = &quot;Ljung-Box&quot;) #Absolute BSE returns ## ## Box-Ljung test ## ## data: ret_BSE_abs ## X-squared = 318.58, df = 1, p-value &lt; 2.2e-16 The same behavior is observed for the US and Japanese market indices Box.test((ret_sp500)^2, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: (ret_sp500)^2 ## X-squared = 115.7, df = 1, p-value &lt; 2.2e-16 Box.test((ret_nikkei)^2, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: (ret_nikkei)^2 ## X-squared = 174.83, df = 1, p-value &lt; 2.2e-16 Hence we may safely conclude that the market indices in question display ARCH effectsi.e., their squared residuals display autocorrelation. 4.6 Autoregressive Conditional Heteroskedasticity (ARCH) The essential idea behind ARCH models is that while the shock or innovation \\(u_t\\) is not autocorrelated, its squared lagged terms do display autocorrelation. In other words, the ARCH(\\(m\\)) process assumes: \\[u_t = \\sigma_t\\epsilon_t\\] \\[\\sigma_t^2 = \\beta_0 + \\beta_1u_{t-1}^2+\\hdots+\\beta_mu_{t-m}^2\\] Here \\(\\epsilon_t\\) is iid with mean 0 and variance 1; and all \\(\\beta_j\\) are non negative, with \\(\\beta_0&gt;0\\). In practice it is common to assume that the shocks \\(\\epsilon_t\\) are from a standard normal, standard \\(T\\) or some other suitable distribution. By construction, in ARCH models, large shocks tend to be followed by other large shocks. 4.6.1 Properties of ARCH(1) Processes Consider the ARCH(1) model: \\[u_t = \\sigma_t\\epsilon_t\\] \\[\\sigma^2_t = \\beta_0 + \\beta_1u_{t-1}^2\\] Clearly the unconditional mean is \\(\\mathbb{E}(\\mathbb{E}(u_t|\\mathbb{F}_{t-1})) = 0\\) and the variance is \\(\\text{var}(u_t)=\\mathbb{E}(\\mathbb{E}(u^2_t|\\mathbb{F}_{t-1})) = \\beta_0+\\beta_1\\mathbb{E}(u_{t-1}^2)\\) and hence \\[\\text{var}(u_t) = \\frac{\\beta_0}{1-\\beta_1}\\] In the same way if we compute higher moments and more specifically, the kurtosis we find that the excess kurtosis of \\(u_t\\) is positive or in other words, for an ARCH(1) process the shocks are more fat-tailedmore likely to produce extreme values. This is consistent with empirical observations. 4.6.2 Order Determination of ARCH(\\(m\\)) Processes The ARCH(\\(m\\)) process is: \\[\\sigma_t^2 = \\beta_0+\\beta_1u_{t-1}^2+\\hdots+\\beta_mu_{t-m}^2\\] Since \\(u_t^2\\) is an unbiased estimator of \\(\\sigma_t^2\\) we can interpret the ARCH(\\(m\\)) process as a special form of an AR(\\(m\\)) process for square errors. If ARCH effects are deemed significant, we can employ the PACFs of squared residuals to find the order of the ARCH series just as we did for AR processes. Hence we apply this idea to the market indices below. pacf(ret_BSE_sq, na.action = na.pass) pacf((ret_sp500)^2, na.action = na.pass) pacf((ret_nikkei)^2, na.action = na.pass) Depending on the distributional assumptions for \\(\\epsilon_t\\) we use different forms of the maximum likelihood estimation for estimating ARCH processes. Additionally to check if the ARCH model is well specified, we notice that \\(\\tilde{u}_t = \\frac{u_t}{\\sigma_t}\\) is an iid sequence. 4.7 Generalized ARCH (GARCH) Processes Usage of GARCH models can shorten the order of the corresponding ARCH models. A GARCH(\\(m,n\\)) process is: \\[u_t = \\sigma_t \\epsilon_t\\] \\[\\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^m \\alpha_iu_{t-i}^2 + \\sum_{j=1}^n \\beta_j\\sigma_{t-j}^2\\] GARCH series can be thought to be ARMA series for the squared error \\(u_t^2\\). GARCH models are consistent with the phenomenon of volatility clustering and thei tail distribution is fatter than that of normals. 4.8 Application: Fitting ARMA GARCH Models We check if the three financial market indices can be fitted satisfactorily to ARMA GARCH processes. 4.8.1 The Bombay Stock Exchange # For lag order indication of MA and AR resp acf(ret_BSE, na.action = na.pass) pacf(ret_BSE, na.action = na.pass) # For lag order of ARCH and GARCH acf(ret_BSE_sq, na.action = na.pass) We will use two main functions for estimating such models. One is the function tseries::garch() which may be used for relatively simple fittings and the other are sets of functions from the package rugarch which offers more options including a diverse set of distributions for the innovations. # Specify an ARMA(1,1) GARCH(1,1) model spec_garch_BSE &lt;- rugarch::ugarchspec() #empty specification # Estimate it est_garch_BSE &lt;- rugarch::ugarchfit(data = !is.na(ret_BSE), #note spec = spec_garch_BSE ) show(est_garch_BSE) ## ## *---------------------------------* ## * GARCH Model Fit * ## *---------------------------------* ## ## Conditional Variance Dynamics ## ----------------------------------- ## GARCH Model : sGARCH(1,1) ## Mean Model : ARFIMA(1,0,1) ## Distribution : norm ## ## Optimal Parameters ## ------------------------------------ ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 1.00000 0.000069 14564.3352 0.000000 ## ar1 0.00000 0.141076 0.0000 1.000000 ## ma1 0.24873 0.160545 1.5493 0.121319 ## omega 0.00000 0.000000 4.6840 0.000003 ## alpha1 0.05000 0.009125 5.4792 0.000000 ## beta1 0.90000 0.001924 467.7716 0.000000 ## ## Robust Standard Errors: ## Estimate Std. Error t value Pr(&gt;|t|) ## mu 1.00000 0.000095 10551.5613 0.00000 ## ar1 0.00000 0.339224 0.0000 1.00000 ## ma1 0.24873 0.185917 1.3378 0.18095 ## omega 0.00000 0.000000 1.3490 0.17734 ## alpha1 0.05000 0.005717 8.7463 0.00000 ## beta1 0.90000 0.003441 261.5554 0.00000 ## ## LogLikelihood : 24036.84 ## ## Information Criteria ## ------------------------------------ ## ## Akaike -11.006 ## Bayes -10.997 ## Shibata -11.006 ## Hannan-Quinn -11.003 ## ## Weighted Ljung-Box Test on Standardized Residuals ## ------------------------------------ ## statistic p-value ## Lag[1] 1.325 0.2497 ## Lag[2*(p+q)+(p+q)-1][5] 1.397 0.9992 ## Lag[4*(p+q)+(p+q)-1][9] 1.406 0.9980 ## d.o.f=2 ## H0 : No serial correlation ## ## Weighted Ljung-Box Test on Standardized Squared Residuals ## ------------------------------------ ## statistic p-value ## Lag[1] 0.0003950 0.9841 ## Lag[2*(p+q)+(p+q)-1][5] 0.0003963 1.0000 ## Lag[4*(p+q)+(p+q)-1][9] 0.0003964 1.0000 ## d.o.f=2 ## ## Weighted ARCH LM Tests ## ------------------------------------ ## Statistic Shape Scale P-Value ## ARCH Lag[3] 5.943e-09 0.500 2.000 0.9999 ## ARCH Lag[5] 6.164e-09 1.440 1.667 1.0000 ## ARCH Lag[7] 6.700e-09 2.315 1.543 1.0000 ## ## Nyblom stability test ## ------------------------------------ ## Joint Statistic: -1907.443 ## Individual Statistics: ## mu 701.24 ## ar1 31.71 ## ma1 34.82 ## omega 1367.22 ## alpha1 103.39 ## beta1 1151.78 ## ## Asymptotic Critical Values (10% 5% 1%) ## Joint Statistic: 1.49 1.68 2.12 ## Individual Statistic: 0.35 0.47 0.75 ## ## Sign Bias Test ## ------------------------------------ ## t-value prob sig ## Sign Bias 3.366e+01 5.475e-221 *** ## Negative Sign Bias 1.172e+03 0.000e+00 *** ## Positive Sign Bias 8.267e+01 0.000e+00 *** ## Joint Effect 1.659e+06 0.000e+00 *** ## ## ## Adjusted Pearson Goodness-of-Fit Test: ## ------------------------------------ ## group statistic p-value(g-1) ## 1 20 82257 0 ## 2 30 125641 0 ## 3 40 169128 0 ## 4 50 212660 0 ## ## ## Elapsed time : 0.218045 4.9 References For a general AR(\\(p\\)) process, the corresponding condition is: \\(|\\phi_1|+|\\phi_2|+\\hdots+|\\phi_p|&lt;1\\). "]]
