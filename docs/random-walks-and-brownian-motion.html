<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Random Walks and Brownian Motion | Stochastic Calculus: An Idiosyncratic Approach</title>
  <meta name="description" content="An idiosyncratic approach to stochastic calculus with applications to mathematical finance." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Random Walks and Brownian Motion | Stochastic Calculus: An Idiosyncratic Approach" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An idiosyncratic approach to stochastic calculus with applications to mathematical finance." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Random Walks and Brownian Motion | Stochastic Calculus: An Idiosyncratic Approach" />
  
  <meta name="twitter:description" content="An idiosyncratic approach to stochastic calculus with applications to mathematical finance." />
  

<meta name="author" content="Abhinav Anand" />


<meta name="date" content="2026-01-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="itô-integral-and-itôs-lemma.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stochastic Calculus from Scratch</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-youll-learn"><i class="fa fa-check"></i>What You’ll Learn</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About This Book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#book-overview"><i class="fa fa-check"></i>Book Overview</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html"><i class="fa fa-check"></i><b>1</b> Random Walks and Brownian Motion</a>
<ul>
<li class="chapter" data-level="1.1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#why-random-walks-matter"><i class="fa fa-check"></i><b>1.1</b> Why Random Walks Matter</a></li>
<li class="chapter" data-level="1.2" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#simple-random-walk"><i class="fa fa-check"></i><b>1.2</b> Simple Random Walk</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#key-properties"><i class="fa fa-check"></i><b>1.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="1.2.2" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#visualizing-random-walk-behavior"><i class="fa fa-check"></i><b>1.2.2</b> Visualizing Random Walk Behavior</a></li>
<li class="chapter" data-level="1.2.3" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-central-limit-theorem-in-action"><i class="fa fa-check"></i><b>1.2.3</b> The Central Limit Theorem in Action</a></li>
<li class="chapter" data-level="1.2.4" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#synthesis-what-weve-learned"><i class="fa fa-check"></i><b>1.2.4</b> Synthesis: What We’ve Learned</a></li>
<li class="chapter" data-level="1.2.5" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#common-pitfalls-and-misconceptions"><i class="fa fa-check"></i><b>1.2.5</b> Common Pitfalls and Misconceptions</a></li>
<li class="chapter" data-level="1.2.6" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#from-discrete-to-continuous-brownian-motion-preview"><i class="fa fa-check"></i><b>1.2.6</b> From Discrete to Continuous: Brownian Motion Preview</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#from-discrete-steps-to-continuous-paths"><i class="fa fa-check"></i><b>1.3</b> From Discrete Steps to Continuous Paths</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-scaling-thought-experiment"><i class="fa fa-check"></i><b>1.3.1</b> The Scaling Thought Experiment</a></li>
<li class="chapter" data-level="1.3.2" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-crucial-scaling-derivation"><i class="fa fa-check"></i><b>1.3.2</b> The Crucial Scaling Derivation</a></li>
<li class="chapter" data-level="1.3.3" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#visualizing-the-convergence"><i class="fa fa-check"></i><b>1.3.3</b> Visualizing the Convergence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#brownian-motion-the-continuous-limit"><i class="fa fa-check"></i><b>1.4</b> Brownian Motion: The Continuous Limit</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#formal-definition"><i class="fa fa-check"></i><b>1.4.1</b> Formal Definition</a></li>
<li class="chapter" data-level="1.4.2" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#understanding-the-properties"><i class="fa fa-check"></i><b>1.4.2</b> Understanding the Properties</a></li>
<li class="chapter" data-level="1.4.3" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#an-important-consequence-quadratic-variation"><i class="fa fa-check"></i><b>1.4.3</b> An Important Consequence: Quadratic Variation</a></li>
<li class="chapter" data-level="1.4.4" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-shocking-fact-nowhere-differentiable"><i class="fa fa-check"></i><b>1.4.4</b> The Shocking Fact: Nowhere Differentiable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#a-crisis-and-an-opportunity-rethinking-calculus"><i class="fa fa-check"></i><b>1.5</b> A Crisis and An Opportunity: Rethinking Calculus</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#why-classical-calculus-fails"><i class="fa fa-check"></i><b>1.5.1</b> Why Classical Calculus Fails</a></li>
<li class="chapter" data-level="1.5.2" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-key-insight-build-from-changes-not-slopes"><i class="fa fa-check"></i><b>1.5.2</b> The Key Insight: Build from Changes, Not Slopes</a></li>
<li class="chapter" data-level="1.5.3" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#the-arithmetic-of-infinitesimals-db2-neq-0"><i class="fa fa-check"></i><b>1.5.3</b> The Arithmetic of Infinitesimals: <span class="math inline">\((dB)^2 \neq 0\)</span></a></li>
<li class="chapter" data-level="1.5.4" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#comparing-classical-and-stochastic-infinitesimals"><i class="fa fa-check"></i><b>1.5.4</b> Comparing Classical and Stochastic Infinitesimals</a></li>
<li class="chapter" data-level="1.5.5" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#quadratic-variation-a-new-concept"><i class="fa fa-check"></i><b>1.5.5</b> Quadratic Variation: A New Concept</a></li>
<li class="chapter" data-level="1.5.6" data-path="random-walks-and-brownian-motion.html"><a href="random-walks-and-brownian-motion.html#a-new-foundation-for-calculus"><i class="fa fa-check"></i><b>1.5.6</b> A New Foundation for Calculus</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html"><i class="fa fa-check"></i><b>2</b> Itô Integral and Itô’s Lemma</a>
<ul>
<li class="chapter" data-level="2.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-challenge-of-stochastic-integration"><i class="fa fa-check"></i><b>2.1</b> The Challenge of Stochastic Integration</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-problem"><i class="fa fa-check"></i><b>2.1.1</b> The Problem</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-it%C3%B4-integral"><i class="fa fa-check"></i><b>2.2</b> The Itô Integral</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#definition"><i class="fa fa-check"></i><b>2.2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2.2" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#why-the-left-endpoint"><i class="fa fa-check"></i><b>2.2.2</b> Why the Left Endpoint?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#a-crucial-example-int_0t-bs-dbs"><i class="fa fa-check"></i><b>2.3</b> A Crucial Example: <span class="math inline">\(\int_0^t B(s) \, dB(s)\)</span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-quadratic-variation-miracle"><i class="fa fa-check"></i><b>2.3.1</b> The Quadratic Variation Miracle</a></li>
<li class="chapter" data-level="2.3.2" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-shock"><i class="fa fa-check"></i><b>2.3.2</b> The Shock</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#quadratic-variation-and-the-formal-rule-db2-dt"><i class="fa fa-check"></i><b>2.4</b> Quadratic Variation and the Formal Rule <span class="math inline">\((dB)^2 = dt\)</span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#multiplication-rules"><i class="fa fa-check"></i><b>2.4.1</b> Multiplication Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#it%C3%B4s-lemma-the-fundamental-theorem"><i class="fa fa-check"></i><b>2.5</b> Itô’s Lemma: The Fundamental Theorem</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-derivation"><i class="fa fa-check"></i><b>2.5.1</b> The Derivation</a></li>
<li class="chapter" data-level="2.5.2" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#it%C3%B4s-lemma-for-fbt"><i class="fa fa-check"></i><b>2.5.2</b> Itô’s Lemma for <span class="math inline">\(f(B(t))\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#verification-recovering-our-earlier-result"><i class="fa fa-check"></i><b>2.6</b> Verification: Recovering Our Earlier Result</a></li>
<li class="chapter" data-level="2.7" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#example-the-exponential"><i class="fa fa-check"></i><b>2.7</b> Example: The Exponential</a></li>
<li class="chapter" data-level="2.8" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#general-it%C3%B4s-lemma"><i class="fa fa-check"></i><b>2.8</b> General Itô’s Lemma</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#the-pattern"><i class="fa fa-check"></i><b>2.8.1</b> The Pattern</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="itô-integral-and-itôs-lemma.html"><a href="itô-integral-and-itôs-lemma.html#why-it%C3%B4s-choice"><i class="fa fa-check"></i><b>2.9</b> Why Itô’s Choice?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html"><i class="fa fa-check"></i><b>3</b> Stochastic Differential Equations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#introduction-to-sdes"><i class="fa fa-check"></i><b>3.1</b> Introduction to SDEs</a></li>
<li class="chapter" data-level="3.2" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#geometric-brownian-motion"><i class="fa fa-check"></i><b>3.2</b> Geometric Brownian Motion</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#interpretation"><i class="fa fa-check"></i><b>3.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.2.2" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#solving-gbm"><i class="fa fa-check"></i><b>3.2.2</b> Solving GBM</a></li>
<li class="chapter" data-level="3.2.3" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#key-observations"><i class="fa fa-check"></i><b>3.2.3</b> Key Observations</a></li>
<li class="chapter" data-level="3.2.4" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#the-it%C3%B4-correction-explained"><i class="fa fa-check"></i><b>3.2.4</b> The Itô Correction Explained</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#ornstein-uhlenbeck-process"><i class="fa fa-check"></i><b>3.3</b> Ornstein-Uhlenbeck Process</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#interpretation-1"><i class="fa fa-check"></i><b>3.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.3.2" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#solving-the-ou-process"><i class="fa fa-check"></i><b>3.3.2</b> Solving the OU Process</a></li>
<li class="chapter" data-level="3.3.3" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#properties-of-the-solution"><i class="fa fa-check"></i><b>3.3.3</b> Properties of the Solution</a></li>
<li class="chapter" data-level="3.3.4" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#generalization-ou-with-mean-mu"><i class="fa fa-check"></i><b>3.3.4</b> Generalization: OU with Mean <span class="math inline">\(\mu\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#general-strategy-for-solving-sdes"><i class="fa fa-check"></i><b>3.4</b> General Strategy for Solving SDEs</a></li>
<li class="chapter" data-level="3.5" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#linear-sdes"><i class="fa fa-check"></i><b>3.5</b> Linear SDEs</a></li>
<li class="chapter" data-level="3.6" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#existence-and-uniqueness"><i class="fa fa-check"></i><b>3.6</b> Existence and Uniqueness</a></li>
<li class="chapter" data-level="3.7" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#strong-vs.-weak-solutions"><i class="fa fa-check"></i><b>3.7</b> Strong vs. Weak Solutions</a></li>
<li class="chapter" data-level="3.8" data-path="stochastic-differential-equations.html"><a href="stochastic-differential-equations.html#multidimensional-sdes"><i class="fa fa-check"></i><b>3.8</b> Multidimensional SDEs</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html"><i class="fa fa-check"></i><b>4</b> Feynman-Kac and PDEs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#the-bridge-between-probability-and-analysis"><i class="fa fa-check"></i><b>4.1</b> The Bridge Between Probability and Analysis</a></li>
<li class="chapter" data-level="4.2" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#the-backward-kolmogorov-equation"><i class="fa fa-check"></i><b>4.2</b> The Backward Kolmogorov Equation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#heuristic-derivation"><i class="fa fa-check"></i><b>4.2.1</b> Heuristic Derivation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#example-the-heat-equation"><i class="fa fa-check"></i><b>4.3</b> Example: The Heat Equation</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#probabilistic-interpretation"><i class="fa fa-check"></i><b>4.3.1</b> Probabilistic Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#the-black-scholes-pde"><i class="fa fa-check"></i><b>4.4</b> The Black-Scholes PDE</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#solving-for-a-european-call"><i class="fa fa-check"></i><b>4.4.1</b> Solving for a European Call</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#the-forward-kolmogorov-equation"><i class="fa fa-check"></i><b>4.5</b> The Forward Kolmogorov Equation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#the-duality"><i class="fa fa-check"></i><b>4.5.1</b> The Duality</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#feynman-kac-formula-general-version"><i class="fa fa-check"></i><b>4.6</b> Feynman-Kac Formula (General Version)</a></li>
<li class="chapter" data-level="4.7" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#applications"><i class="fa fa-check"></i><b>4.7</b> Applications</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#option-pricing"><i class="fa fa-check"></i><b>4.7.1</b> Option Pricing</a></li>
<li class="chapter" data-level="4.7.2" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#statistical-mechanics"><i class="fa fa-check"></i><b>4.7.2</b> Statistical Mechanics</a></li>
<li class="chapter" data-level="4.7.3" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#control-theory"><i class="fa fa-check"></i><b>4.7.3</b> Control Theory</a></li>
<li class="chapter" data-level="4.7.4" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#biology"><i class="fa fa-check"></i><b>4.7.4</b> Biology</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#numerical-methods"><i class="fa fa-check"></i><b>4.8</b> Numerical Methods</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>4.8.1</b> Monte Carlo Simulation</a></li>
<li class="chapter" data-level="4.8.2" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#pde-methods"><i class="fa fa-check"></i><b>4.8.2</b> PDE Methods</a></li>
<li class="chapter" data-level="4.8.3" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#hybrid-methods"><i class="fa fa-check"></i><b>4.8.3</b> Hybrid Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="feynman-kac-and-pdes.html"><a href="feynman-kac-and-pdes.html#summary"><i class="fa fa-check"></i><b>4.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html"><i class="fa fa-check"></i><b>5</b> Risk-Neutral Pricing and Martingales</a>
<ul>
<li class="chapter" data-level="5.1" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#martingales-fair-games"><i class="fa fa-check"></i><b>5.1</b> Martingales: Fair Games</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#examples"><i class="fa fa-check"></i><b>5.1.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#the-fundamental-theorem-of-asset-pricing"><i class="fa fa-check"></i><b>5.2</b> The Fundamental Theorem of Asset Pricing</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#whats-arbitrage"><i class="fa fa-check"></i><b>5.2.1</b> What’s Arbitrage?</a></li>
<li class="chapter" data-level="5.2.2" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#the-risk-neutral-measure"><i class="fa fa-check"></i><b>5.2.2</b> The Risk-Neutral Measure</a></li>
<li class="chapter" data-level="5.2.3" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#girsanovs-theorem"><i class="fa fa-check"></i><b>5.2.3</b> Girsanov’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#risk-neutral-pricing-formula"><i class="fa fa-check"></i><b>5.3</b> Risk-Neutral Pricing Formula</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#the-miracle"><i class="fa fa-check"></i><b>5.3.1</b> The Miracle</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#why-does-this-work"><i class="fa fa-check"></i><b>5.4</b> Why Does This Work?</a></li>
<li class="chapter" data-level="5.5" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#example-forward-contracts"><i class="fa fa-check"></i><b>5.5</b> Example: Forward Contracts</a></li>
<li class="chapter" data-level="5.6" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#change-of-numeraire"><i class="fa fa-check"></i><b>5.6</b> Change of Numeraire</a></li>
<li class="chapter" data-level="5.7" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#market-price-of-risk"><i class="fa fa-check"></i><b>5.7</b> Market Price of Risk</a></li>
<li class="chapter" data-level="5.8" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#incomplete-markets"><i class="fa fa-check"></i><b>5.8</b> Incomplete Markets</a></li>
<li class="chapter" data-level="5.9" data-path="risk-neutral-pricing-and-martingales.html"><a href="risk-neutral-pricing-and-martingales.html#summary-1"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html"><i class="fa fa-check"></i><b>6</b> Black-Scholes Theory</a>
<ul>
<li class="chapter" data-level="6.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-black-scholes-formula"><i class="fa fa-check"></i><b>6.1</b> The Black-Scholes Formula</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#derivation"><i class="fa fa-check"></i><b>6.1.1</b> Derivation</a></li>
<li class="chapter" data-level="6.1.2" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#interpretation-2"><i class="fa fa-check"></i><b>6.1.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-greeks"><i class="fa fa-check"></i><b>6.2</b> The Greeks</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#delta-delta"><i class="fa fa-check"></i><b>6.2.1</b> Delta (<span class="math inline">\(\Delta\)</span>)</a></li>
<li class="chapter" data-level="6.2.2" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#gamma-gamma"><i class="fa fa-check"></i><b>6.2.2</b> Gamma (<span class="math inline">\(\Gamma\)</span>)</a></li>
<li class="chapter" data-level="6.2.3" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#vega-mathcalv"><i class="fa fa-check"></i><b>6.2.3</b> Vega (<span class="math inline">\(\mathcal{V}\)</span>)</a></li>
<li class="chapter" data-level="6.2.4" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#theta-theta"><i class="fa fa-check"></i><b>6.2.4</b> Theta (<span class="math inline">\(\Theta\)</span>)</a></li>
<li class="chapter" data-level="6.2.5" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#rho-rho"><i class="fa fa-check"></i><b>6.2.5</b> Rho (<span class="math inline">\(\rho\)</span>)</a></li>
<li class="chapter" data-level="6.2.6" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-greeks-relationship"><i class="fa fa-check"></i><b>6.2.6</b> The Greeks Relationship</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#delta-hedging"><i class="fa fa-check"></i><b>6.3</b> Delta Hedging</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-strategy"><i class="fa fa-check"></i><b>6.3.1</b> The Strategy</a></li>
<li class="chapter" data-level="6.3.2" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#discrete-hedging-pl"><i class="fa fa-check"></i><b>6.3.2</b> Discrete Hedging P&amp;L</a></li>
<li class="chapter" data-level="6.3.3" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-catch"><i class="fa fa-check"></i><b>6.3.3</b> The Catch</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#put-call-parity"><i class="fa fa-check"></i><b>6.4</b> Put-Call Parity</a></li>
<li class="chapter" data-level="6.5" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#implied-volatility"><i class="fa fa-check"></i><b>6.5</b> Implied Volatility</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#the-volatility-smile"><i class="fa fa-check"></i><b>6.5.1</b> The Volatility Smile</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#american-options"><i class="fa fa-check"></i><b>6.6</b> American Options</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#early-exercise-premium"><i class="fa fa-check"></i><b>6.6.1</b> Early Exercise Premium</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#extensions"><i class="fa fa-check"></i><b>6.7</b> Extensions</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#dividends"><i class="fa fa-check"></i><b>6.7.1</b> Dividends</a></li>
<li class="chapter" data-level="6.7.2" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#exotic-options"><i class="fa fa-check"></i><b>6.7.2</b> Exotic Options</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="black-scholes-theory.html"><a href="black-scholes-theory.html#limitations-of-black-scholes"><i class="fa fa-check"></i><b>6.8</b> Limitations of Black-Scholes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html"><i class="fa fa-check"></i><b>7</b> Jump-Diffusion Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#why-jumps-matter"><i class="fa fa-check"></i><b>7.1</b> Why Jumps Matter</a></li>
<li class="chapter" data-level="7.2" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#the-poisson-process"><i class="fa fa-check"></i><b>7.2</b> The Poisson Process</a></li>
<li class="chapter" data-level="7.3" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#compound-poisson-process"><i class="fa fa-check"></i><b>7.3</b> Compound Poisson Process</a></li>
<li class="chapter" data-level="7.4" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#mertons-jump-diffusion-model"><i class="fa fa-check"></i><b>7.4</b> Merton’s Jump-Diffusion Model</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#why-ey---1"><i class="fa fa-check"></i><b>7.4.1</b> Why <span class="math inline">\(e^Y - 1\)</span>?</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#it%C3%B4s-lemma-with-jumps"><i class="fa fa-check"></i><b>7.5</b> Itô’s Lemma with Jumps</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#example-log-price"><i class="fa fa-check"></i><b>7.5.1</b> Example: Log Price</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#risk-neutral-pricing-with-jumps"><i class="fa fa-check"></i><b>7.6</b> Risk-Neutral Pricing with Jumps</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#the-critical-insight"><i class="fa fa-check"></i><b>7.6.1</b> The Critical Insight</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#mertons-option-pricing-formula"><i class="fa fa-check"></i><b>7.7</b> Merton’s Option Pricing Formula</a></li>
<li class="chapter" data-level="7.8" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#the-pide"><i class="fa fa-check"></i><b>7.8</b> The PIDE</a></li>
<li class="chapter" data-level="7.9" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#market-implications"><i class="fa fa-check"></i><b>7.9</b> Market Implications</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#volatility-smile"><i class="fa fa-check"></i><b>7.9.1</b> Volatility Smile</a></li>
<li class="chapter" data-level="7.9.2" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#greeks"><i class="fa fa-check"></i><b>7.9.2</b> Greeks</a></li>
<li class="chapter" data-level="7.9.3" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#calibration"><i class="fa fa-check"></i><b>7.9.3</b> Calibration</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#extensions-1"><i class="fa fa-check"></i><b>7.10</b> Extensions</a>
<ul>
<li class="chapter" data-level="7.10.1" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#double-exponential-jumps"><i class="fa fa-check"></i><b>7.10.1</b> Double exponential jumps</a></li>
<li class="chapter" data-level="7.10.2" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#stochastic-intensity"><i class="fa fa-check"></i><b>7.10.2</b> Stochastic intensity</a></li>
<li class="chapter" data-level="7.10.3" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#jump-to-default"><i class="fa fa-check"></i><b>7.10.3</b> Jump-to-default</a></li>
</ul></li>
<li class="chapter" data-level="7.11" data-path="jump-diffusion-models.html"><a href="jump-diffusion-models.html#summary-2"><i class="fa fa-check"></i><b>7.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lévy-processes.html"><a href="lévy-processes.html"><i class="fa fa-check"></i><b>8</b> Lévy Processes</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lévy-processes.html"><a href="lévy-processes.html#the-ultimate-generalization"><i class="fa fa-check"></i><b>8.1</b> The Ultimate Generalization</a></li>
<li class="chapter" data-level="8.2" data-path="lévy-processes.html"><a href="lévy-processes.html#examples-we-know"><i class="fa fa-check"></i><b>8.2</b> Examples We Know</a></li>
<li class="chapter" data-level="8.3" data-path="lévy-processes.html"><a href="lévy-processes.html#the-l%C3%A9vy-khintchine-representation"><i class="fa fa-check"></i><b>8.3</b> The Lévy-Khintchine Representation</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="lévy-processes.html"><a href="lévy-processes.html#the-l%C3%A9vy-khintchine-formula"><i class="fa fa-check"></i><b>8.3.1</b> The Lévy-Khintchine Formula</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="lévy-processes.html"><a href="lévy-processes.html#types-of-jump-behavior"><i class="fa fa-check"></i><b>8.4</b> Types of Jump Behavior</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lévy-processes.html"><a href="lévy-processes.html#finite-activity"><i class="fa fa-check"></i><b>8.4.1</b> Finite Activity</a></li>
<li class="chapter" data-level="8.4.2" data-path="lévy-processes.html"><a href="lévy-processes.html#infinite-activity"><i class="fa fa-check"></i><b>8.4.2</b> Infinite Activity</a></li>
<li class="chapter" data-level="8.4.3" data-path="lévy-processes.html"><a href="lévy-processes.html#finite-variation"><i class="fa fa-check"></i><b>8.4.3</b> Finite Variation</a></li>
<li class="chapter" data-level="8.4.4" data-path="lévy-processes.html"><a href="lévy-processes.html#infinite-variation"><i class="fa fa-check"></i><b>8.4.4</b> Infinite Variation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="lévy-processes.html"><a href="lévy-processes.html#popular-l%C3%A9vy-processes-in-finance"><i class="fa fa-check"></i><b>8.5</b> Popular Lévy Processes in Finance</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="lévy-processes.html"><a href="lévy-processes.html#variance-gamma-vg"><i class="fa fa-check"></i><b>8.5.1</b> Variance Gamma (VG)</a></li>
<li class="chapter" data-level="8.5.2" data-path="lévy-processes.html"><a href="lévy-processes.html#normal-inverse-gaussian-nig"><i class="fa fa-check"></i><b>8.5.2</b> Normal Inverse Gaussian (NIG)</a></li>
<li class="chapter" data-level="8.5.3" data-path="lévy-processes.html"><a href="lévy-processes.html#cgmy-carr-geman-madan-yor"><i class="fa fa-check"></i><b>8.5.3</b> CGMY (Carr-Geman-Madan-Yor)</a></li>
<li class="chapter" data-level="8.5.4" data-path="lévy-processes.html"><a href="lévy-processes.html#tempered-stable-processes"><i class="fa fa-check"></i><b>8.5.4</b> Tempered Stable Processes</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="lévy-processes.html"><a href="lévy-processes.html#why-l%C3%A9vy-processes-matter-for-finance"><i class="fa fa-check"></i><b>8.6</b> Why Lévy Processes Matter for Finance</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="lévy-processes.html"><a href="lévy-processes.html#empirical-fit"><i class="fa fa-check"></i><b>8.6.1</b> Empirical Fit</a></li>
<li class="chapter" data-level="8.6.2" data-path="lévy-processes.html"><a href="lévy-processes.html#infinite-divisibility"><i class="fa fa-check"></i><b>8.6.2</b> Infinite Divisibility</a></li>
<li class="chapter" data-level="8.6.3" data-path="lévy-processes.html"><a href="lévy-processes.html#tractability"><i class="fa fa-check"></i><b>8.6.3</b> Tractability</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="lévy-processes.html"><a href="lévy-processes.html#pricing-with-l%C3%A9vy-processes"><i class="fa fa-check"></i><b>8.7</b> Pricing with Lévy Processes</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="lévy-processes.html"><a href="lévy-processes.html#characteristic-function-method"><i class="fa fa-check"></i><b>8.7.1</b> Characteristic Function Method</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="lévy-processes.html"><a href="lévy-processes.html#the-l%C3%A9vy-landscape"><i class="fa fa-check"></i><b>8.8</b> The Lévy Landscape</a></li>
<li class="chapter" data-level="8.9" data-path="lévy-processes.html"><a href="lévy-processes.html#subordination"><i class="fa fa-check"></i><b>8.9</b> Subordination</a></li>
<li class="chapter" data-level="8.10" data-path="lévy-processes.html"><a href="lévy-processes.html#limitations-and-extensions"><i class="fa fa-check"></i><b>8.10</b> Limitations and Extensions</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="lévy-processes.html"><a href="lévy-processes.html#what-l%C3%A9vy-processes-cannot-capture"><i class="fa fa-check"></i><b>8.10.1</b> What Lévy processes cannot capture</a></li>
<li class="chapter" data-level="8.10.2" data-path="lévy-processes.html"><a href="lévy-processes.html#modern-extensions"><i class="fa fa-check"></i><b>8.10.2</b> Modern extensions</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="lévy-processes.html"><a href="lévy-processes.html#the-full-circle"><i class="fa fa-check"></i><b>8.11</b> The Full Circle</a></li>
<li class="chapter" data-level="8.12" data-path="lévy-processes.html"><a href="lévy-processes.html#further-directions"><i class="fa fa-check"></i><b>8.12</b> Further Directions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>A</b> Essential Linear Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#why-geometry-matters"><i class="fa fa-check"></i>Why Geometry Matters</a></li>
<li class="chapter" data-level="A.1" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors-and-linear-transformations"><i class="fa fa-check"></i><b>A.1</b> Vectors and Linear Transformations</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors-as-arrows"><i class="fa fa-check"></i>Vectors as Arrows</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#matrices-as-transformations"><i class="fa fa-check"></i>Matrices as Transformations</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#what-can-linear-transformations-do"><i class="fa fa-check"></i>What Can Linear Transformations Do?</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="linear-algebra.html"><a href="linear-algebra.html#eigendecomposition-finding-the-fixed-directions"><i class="fa fa-check"></i><b>A.2</b> Eigendecomposition: Finding the Fixed Directions</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-central-question"><i class="fa fa-check"></i>The Central Question</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#eigenvectors-as-fixed-directions"><i class="fa fa-check"></i>Eigenvectors as Fixed Directions</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-eigendecomposition"><i class="fa fa-check"></i>The Eigendecomposition</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#special-case-symmetric-matrices"><i class="fa fa-check"></i>Special Case: Symmetric Matrices</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="linear-algebra.html"><a href="linear-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.3</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#beyond-square-matrices"><i class="fa fa-check"></i>Beyond Square Matrices</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-svd-theorem"><i class="fa fa-check"></i>The SVD Theorem</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-geometric-revelation"><i class="fa fa-check"></i>The Geometric Revelation</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#connection-to-eigendecomposition"><i class="fa fa-check"></i>Connection to Eigendecomposition</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#geometric-interpretation-of-singular-values"><i class="fa fa-check"></i>Geometric Interpretation of Singular Values</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#low-rank-approximation"><i class="fa fa-check"></i>Low-Rank Approximation</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="linear-algebra.html"><a href="linear-algebra.html#implications-principal-component-analysis"><i class="fa fa-check"></i><b>A.4</b> Implications: Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-setup"><i class="fa fa-check"></i>The Setup</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-geometric-view"><i class="fa fa-check"></i>The Geometric View</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#principal-components"><i class="fa fa-check"></i>Principal Components</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#the-svd-connection"><i class="fa fa-check"></i>The SVD Connection</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#why-pca-matters-for-finance"><i class="fa fa-check"></i>Why PCA Matters for Finance</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#a-caution"><i class="fa fa-check"></i>A Caution</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/abhinavananddwivedi/book" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stochastic Calculus: An Idiosyncratic Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-walks-and-brownian-motion" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Random Walks and Brownian Motion<a href="random-walks-and-brownian-motion.html#random-walks-and-brownian-motion" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In 1827, botanist Robert Brown observed pollen grains suspended in water through his microscope, jittering ceaselessly in an erratic dance. Nearly eighty years later, Einstein explained this <em>Brownian motion</em>: each grain is bombarded by countless invisible water molecules, accumulating random kicks that send it on a wandering path—a continuous random walk. Yet here’s the puzzle that should make you pause: in 1900, five years <em>before</em> Einstein’s physics paper, a French mathematician named Louis Bachelier wrote his doctoral thesis describing stock price movements using the exact same mathematical framework. How could the physics of microscopic particles and the economics of human market behavior—phenomena separated by scales of size, time, and causation—obey identical mathematical laws? The answer hints at a deep universality: whenever a quantity evolves through the accumulation of many small, independent, unpredictable shocks—whether molecular collisions or market trades—the Central Limit Theorem sculpts the result into the same characteristic shape, regardless of the underlying details.</p>
<div id="why-random-walks-matter" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Why Random Walks Matter<a href="random-walks-and-brownian-motion.html#why-random-walks-matter" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine a particle suspended in fluid, buffeted by molecular collisions. Or the path of a stock price over time, influenced by countless buy and sell decisions. Or even the position of a gambler’s fortune after many coin flips. What unifies these seemingly disparate phenomena? They all exhibit <strong>random walk</strong> behavior.</p>
<p>Random walks are fundamental models in probability theory with applications spanning physics (diffusion processes), finance (asset price modeling), biology (animal foraging), and computer science (randomized algorithms). Understanding random walks gives us the key to analyzing systems where change occurs through accumulated random shocks.</p>
<p><strong>This chapter’s journey:</strong> We’ll begin with the simplest random walk—a sequence of coin flips—and discover that when properly scaled, these discrete jumps converge to a continuous process called <strong>Brownian motion</strong>. This limiting process is not just mathematically elegant; it’s the foundation of stochastic calculus and modern quantitative finance.</p>
<p>Three questions will guide us:
1. How does a random walk behave over time?
2. What happens when we standardize it?
3. How do we transition from discrete steps to continuous motion?</p>
</div>
<div id="simple-random-walk" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Simple Random Walk<a href="random-walks-and-brownian-motion.html#simple-random-walk" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine flipping a fair coin repeatedly. After each flip, you take a step: heads = +1, tails = -1. Your position after <span class="math inline">\(n\)</span> steps is:
<span class="math display">\[S_n = X_1 + X_2 + \cdots + X_n\]</span>
where each <span class="math inline">\(X_i \in \{-1, +1\}\)</span> with equal probability <span class="math inline">\(1/2\)</span>. This is a <strong>simple random walk</strong>.</p>
<p><strong>Intuition:</strong> Think of <span class="math inline">\(S_n\)</span> as your “signed distance” from where you started. Each coin flip adds a random <span class="math inline">\(\pm 1\)</span> to your position. Since the coin is fair, there’s no systematic drift in any direction—but randomness ensures you won’t stay at zero either.</p>
<div id="key-properties" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Key Properties<a href="random-walks-and-brownian-motion.html#key-properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve defined the random walk, let’s examine what makes it tick. Three properties will be crucial for understanding its long-term behavior:</p>
<ul>
<li><p><strong>Expected position</strong>: <span class="math inline">\(\mathbb{E}[S_n] = 0\)</span></p>
<p><em>Interpretation:</em> On average, you expect to end where you started. The walk has no drift or bias.</p></li>
<li><p><strong>Variance</strong>: <span class="math inline">\(\text{Var}(S_n) = n\)</span></p>
<p><em>Interpretation:</em> The variance grows linearly with time. The typical distance from the origin grows, but crucially, it grows as <span class="math inline">\(\sqrt{n}\)</span>, not as <span class="math inline">\(n\)</span> itself.</p></li>
<li><p><strong>Asymptotic normality</strong>: <span class="math inline">\(\frac{S_n}{\sqrt{n}} \xrightarrow{d} N(0,1)\)</span> as <span class="math inline">\(n \to \infty\)</span></p>
<p><em>Interpretation:</em> By the Central Limit Theorem, even though each step is discrete (<span class="math inline">\(\pm 1\)</span>), the rescaled position becomes approximately normal for large <span class="math inline">\(n\)</span>. Here <span class="math inline">\(\xrightarrow{d}\)</span> denotes convergence in distribution.</p></li>
</ul>
<p><strong>Why these properties matter:</strong> The first two tell us about the walk’s center and spread. The third—asymptotic normality—is profound: it means that regardless of the distribution of individual steps, their sum (properly scaled) becomes Gaussian. This universality is what makes random walks so powerful as models.</p>
<blockquote>
<p><strong>💭 Thought Exercise:</strong> Why does variance grow linearly with <span class="math inline">\(n\)</span> but standard deviation grow as <span class="math inline">\(\sqrt{n}\)</span>? What does this imply about how “surprising” a deviation of size <span class="math inline">\(k\)</span> becomes as time progresses?</p>
<p><em>Hint:</em> Since <span class="math inline">\(\text{SD}(S_n) = \sqrt{n}\)</span>, a deviation of fixed size <span class="math inline">\(k\)</span> represents <span class="math inline">\(k/\sqrt{n}\)</span> standard deviations. As <span class="math inline">\(n\)</span> increases, the same absolute deviation becomes less and less surprising in relative terms.</p>
</blockquote>
</div>
<div id="visualizing-random-walk-behavior" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Visualizing Random Walk Behavior<a href="random-walks-and-brownian-motion.html#visualizing-random-walk-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
Theory is one thing, but seeing is believing. The following simulations will demonstrate each property we just discussed. As you examine each plot, ask yourself:
- Does the behavior match our mathematical predictions?
- What patterns emerge as time increases?
- Can you visually identify the <span class="math inline">\(\sqrt{n}\)</span> scaling in the spread?
<div class="figure" style="text-align: center">
<img src="figures/random_walk_simulate.png" alt="Five independent random walk paths over 1000 steps. Each colored line represents one realization of the random walk process." width="80%" />
<p class="caption">
(#fig:rw_simulate)Five independent random walk paths over 1000 steps. Each colored line represents one realization of the random walk process.
</p>
</div>
<strong>What we observe:</strong> Each path wanders unpredictably—sometimes venturing far from zero, other times hovering nearby. Yet they all cluster around zero on average. This is <span class="math inline">\(\mathbb{E}[S_n] = 0\)</span> in action. Notice how the spread of paths increases over time—that’s the <span class="math inline">\(\text{Var}(S_n) = n\)</span> property manifesting visually. By <span class="math inline">\(n = 1000\)</span>, paths range roughly from <span class="math inline">\(-60\)</span> to <span class="math inline">\(+60\)</span>, which aligns with our prediction: we expect typical deviations of about <span class="math inline">\(\pm 2\sqrt{1000} \approx \pm 63\)</span> (two standard deviations).
<div class="figure" style="text-align: center">
<img src="figures/random_sd_sqrt_n.png" alt="Random walk volatility envelope. The dashed lines show $\pm 1$ standard deviation bounds, which grow as $\sqrt{n}$." width="80%" />
<p class="caption">
(#fig:rw_sqrt)Random walk volatility envelope. The dashed lines show <span class="math inline">\(\pm 1\)</span> standard deviation bounds, which grow as <span class="math inline">\(\sqrt{n}\)</span>.
</p>
</div>
<p><strong>Theory predicts:</strong> The standard deviation envelope should grow as <span class="math inline">\(\sqrt{n}\)</span>. At <span class="math inline">\(n = 100\)</span>, we expect <span class="math inline">\(\text{SD} \approx 10\)</span>. At <span class="math inline">\(n = 400\)</span>, we expect <span class="math inline">\(\text{SD} \approx 20\)</span> (doubling the time quadruples the variance but only doubles the standard deviation). At <span class="math inline">\(n = 1000\)</span>, we expect <span class="math inline">\(\text{SD} \approx 31.6\)</span>.</p>
<p><strong>What the plot confirms:</strong> The <span class="math inline">\(\pm 1\sigma\)</span> envelope (dashed lines) indeed follows the <span class="math inline">\(\sqrt{n}\)</span> curve. The sample path wanders within and occasionally beyond this envelope, exactly as we’d expect from a process whose deviations follow a normal distribution (recall that roughly 68% of a normal distribution lies within <span class="math inline">\(\pm 1\sigma\)</span>).</p>
<strong>Empirical verification:</strong> Simulating many random walks allows us to check whether sample means and variances match theoretical predictions.
<div class="figure" style="text-align: center">
<img src="figures/random_walk_mean.png" alt="Empirical mean of 10,000 random walk simulations at each time point. The mean hovers near zero, confirming $\mathbb{E}[S_n] = 0$." width="80%" />
<p class="caption">
(#fig:rw_mean)Empirical mean of 10,000 random walk simulations at each time point. The mean hovers near zero, confirming <span class="math inline">\(\mathbb{E}[S_n] = 0\)</span>.
</p>
</div>
<strong>What we see:</strong> The empirical mean (blue line) fluctuates slightly around zero due to Monte Carlo noise, but remains remarkably close to the theoretical value. This is the Law of Large Numbers at work: averaging over 10,000 independent walks gives us an accurate estimate of the true expectation.
<div class="figure" style="text-align: center">
<img src="figures/random_walk_variance.png" alt="Empirical variance of 10,000 random walk simulations. The variance grows linearly with time, confirming $\text{Var}(S_n) = n$." width="80%" />
<p class="caption">
(#fig:rw_variance)Empirical variance of 10,000 random walk simulations. The variance grows linearly with time, confirming <span class="math inline">\(\text{Var}(S_n) = n\)</span>.
</p>
</div>
<p><strong>Quantitative check:</strong> At <span class="math inline">\(n = 500\)</span>, we predict <span class="math inline">\(\text{Var}(S_n) = 500\)</span>. The empirical variance tracks the 45-degree line (where variance equals time) almost perfectly. Any small deviations are due to finite-sample variation—with 10,000 simulations, our estimates are quite precise.</p>
</div>
<div id="the-central-limit-theorem-in-action" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> The Central Limit Theorem in Action<a href="random-walks-and-brownian-motion.html#the-central-limit-theorem-in-action" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now for the most important property: asymptotic normality. If we standardize the random walk by subtracting its mean (which is 0) and dividing by its standard deviation (which is <span class="math inline">\(\sqrt{n}\)</span>), the resulting distribution should approach the standard normal:</p>
<p><span class="math display">\[Z_n := \frac{S_n - 0}{\sqrt{n}} = \frac{S_n}{\sqrt{n}} \xrightarrow{d} N(0,1)\]</span></p>
<strong>Why this rescaling?</strong> Dividing by <span class="math inline">\(\sqrt{n}\)</span> (not <span class="math inline">\(n\)</span>) is crucial. If we divided by <span class="math inline">\(n\)</span>, the random walk would vanish: <span class="math inline">\(S_n/n \to 0\)</span>. If we didn’t divide at all, <span class="math inline">\(S_n\)</span> would diverge. The <span class="math inline">\(\sqrt{n}\)</span> factor is the “Goldilocks” scaling that keeps the variance at exactly 1 and allows convergence to a non-degenerate limit.
<div class="figure" style="text-align: center">
<img src="figures/random_std_clt.png" alt="Histograms of standardized random walks at various time points. As $n$ increases, the empirical distribution (blue bars) converges to the standard normal density (red curve)." width="80%" />
<p class="caption">
(#fig:rw_clt)Histograms of standardized random walks at various time points. As <span class="math inline">\(n\)</span> increases, the empirical distribution (blue bars) converges to the standard normal density (red curve).
</p>
</div>
<strong>Visual convergence:</strong> At <span class="math inline">\(n = 10\)</span>, the histogram is somewhat jagged and discrete-looking. By <span class="math inline">\(n = 100\)</span>, it’s smoother and closer to the normal curve. At <span class="math inline">\(n = 1000\)</span>, the fit is nearly perfect. This is the Central Limit Theorem before your eyes: sums of independent random variables, when properly normalized, converge to the normal distribution.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq"></span>
<img src="figures/random_qq.png" alt="Quantile-quantile (Q-Q) plot comparing empirical quantiles of $S_n/\sqrt{n}$ to theoretical quantiles of $N(0,1)$. Points falling on the diagonal indicate distributional agreement." width="80%" />
<p class="caption">
Figure 1.1: Quantile-quantile (Q-Q) plot comparing empirical quantiles of <span class="math inline">\(S_n/\sqrt{n}\)</span> to theoretical quantiles of <span class="math inline">\(N(0,1)\)</span>. Points falling on the diagonal indicate distributional agreement.
</p>
</div>
<p><strong>The Q-Q plot diagnostic:</strong> This plot compares quantiles directly. If the standardized random walk were exactly normal, all points would fall on the diagonal line. We see excellent agreement, especially in the central range (<span class="math inline">\(-2\)</span> to <span class="math inline">\(+2\)</span>). Some deviation in the extreme tails is normal—after all, we’re working with finite samples and approximations.</p>
</div>
<div id="synthesis-what-weve-learned" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Synthesis: What We’ve Learned<a href="random-walks-and-brownian-motion.html#synthesis-what-weve-learned" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s review the visual and mathematical evidence:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Zero drift:</strong> Random walks with symmetric steps (equal probability of <span class="math inline">\(\pm 1\)</span>) have expected value zero at all times.</p></li>
<li><p><strong>Linear variance growth:</strong> The variance grows as <span class="math inline">\(n\)</span>, which means standard deviation grows as <span class="math inline">\(\sqrt{n}\)</span>. The “uncertainty region” widens, but sub-linearly.</p></li>
<li><p><strong>Convergence to normality:</strong> The standardized random walk’s empirical distribution converges to the standard normal density. Mathematically:
<span class="math display">\[\begin{align}
\frac{S_n - 0}{\sqrt{n}} &amp;\xrightarrow{d} N(0,1)\\
\Rightarrow \quad S_n &amp;\xrightarrow{d} \sqrt{n} \cdot N(0,1)
\end{align}\]</span></p></li>
</ol>
<p><strong>Practical implication:</strong> For large <span class="math inline">\(n\)</span>, we can approximate probabilities for <span class="math inline">\(S_n\)</span> using the normal distribution. For example:
<span class="math display">\[P(S_n \leq x) \approx \Phi\left(\frac{x}{\sqrt{n}}\right)\]</span>
where <span class="math inline">\(\Phi\)</span> is the standard normal cumulative distribution function.</p>
</div>
<div id="common-pitfalls-and-misconceptions" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Common Pitfalls and Misconceptions<a href="random-walks-and-brownian-motion.html#common-pitfalls-and-misconceptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before moving forward, let’s address some common sources of confusion:</p>
<ul>
<li><p><strong>❌ Misconception:</strong> “The random walk will eventually return to zero.”</p>
<p><strong>✓ Reality:</strong> While it’s true that <span class="math inline">\(\mathbb{E}[S_n] = 0\)</span> for all <span class="math inline">\(n\)</span>, this doesn’t mean <span class="math inline">\(S_n\)</span> itself will be zero at any particular time. In fact, for a simple random walk, the probability of <em>ever</em> returning to zero is 1 (a beautiful result!), but there’s no guarantee it happens within any finite time window.</p></li>
<li><p><strong>❌ Misconception:</strong> “I should standardize by dividing by <span class="math inline">\(n\)</span>.”</p>
<p><strong>✓ Reality:</strong> Dividing <span class="math inline">\(S_n\)</span> by <span class="math inline">\(n\)</span> gives <span class="math inline">\(S_n/n \to 0\)</span> (by the Law of Large Numbers), which is trivial. The correct scaling is <span class="math inline">\(S_n/\sqrt{n}\)</span>, which preserves variance and yields a meaningful limit.</p></li>
<li><p><strong>❌ Misconception:</strong> “The next step depends on previous steps.”</p>
<p><strong>✓ Reality:</strong> The random walk is <strong>memoryless</strong>—each <span class="math inline">\(X_i\)</span> is independent. While <span class="math inline">\(S_n\)</span> certainly depends on past steps (it’s their sum!), the <em>increment</em> <span class="math inline">\(X_{n+1}\)</span> does not. This independence is crucial for many theoretical results.</p></li>
<li><p><strong>❌ Misconception:</strong> “Since the walk is symmetric, it spends equal time above and below zero.”</p>
<p><strong>✓ Reality:</strong> This is false for finite <span class="math inline">\(n\)</span>! The walk can exhibit long excursions on one side. However, the <strong>arcsine law</strong> tells us that over long times, the proportion of time spent positive converges to a surprisingly non-uniform distribution.</p></li>
</ul>
</div>
<div id="from-discrete-to-continuous-brownian-motion-preview" class="section level3 hasAnchor" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> From Discrete to Continuous: Brownian Motion Preview<a href="random-walks-and-brownian-motion.html#from-discrete-to-continuous-brownian-motion-preview" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ve seen that random walks, when properly scaled, converge to normal distributions at each fixed time <span class="math inline">\(n\)</span>. But what if we wanted a <strong>continuous-time</strong> process?</p>
<p>Here’s the key insight: imagine taking steps more and more frequently, but making each step proportionally smaller. Specifically:</p>
<ul>
<li>Divide the time interval <span class="math inline">\([0, t]\)</span> into <span class="math inline">\(n\)</span> subintervals of length <span class="math inline">\(\Delta t = t/n\)</span></li>
<li>At each subinterval, take a step of size <span class="math inline">\(\pm \sqrt{\Delta t}\)</span> (not <span class="math inline">\(\pm 1\)</span>!)</li>
<li>Let <span class="math inline">\(n \to \infty\)</span></li>
</ul>
<p>This limiting process—called <strong>Donsker’s invariance principle</strong>—yields <strong>Brownian motion</strong> <span class="math inline">\(B(t)\)</span>, a continuous-time stochastic process with remarkable properties:</p>
<ul>
<li><span class="math inline">\(B(0) = 0\)</span> (starts at the origin)</li>
<li><strong>Independent increments:</strong> <span class="math inline">\(B(t) - B(s)\)</span> is independent of the past for <span class="math inline">\(t &gt; s\)</span></li>
<li><strong>Stationary increments:</strong> <span class="math inline">\(B(t) - B(s) \sim N(0, t-s)\)</span> (depends only on time difference)</li>
<li><strong>Continuous paths:</strong> <span class="math inline">\(B(t)\)</span> is continuous in <span class="math inline">\(t\)</span> (though nowhere differentiable!)</li>
</ul>
<p><strong>Visual intuition:</strong> If you plotted random walks with <span class="math inline">\(n = 10, 100, 1000, 10000\)</span> steps (each step scaled by <span class="math inline">\(1/\sqrt{n}\)</span>), you’d see the paths become increasingly continuous. The limit is Brownian motion—a continuous but highly irregular function.</p>
<p><strong>Why this matters:</strong> Brownian motion is the continuous analog of the random walk and serves as the foundation for:
- <strong>Stochastic calculus:</strong> Integration and differentiation with respect to random processes
- <strong>Financial modeling:</strong> The Black-Scholes model assumes stock prices follow geometric Brownian motion
- <strong>Partial differential equations:</strong> Solutions to the heat equation can be represented as expectations over Brownian paths</p>
<p>In the next sections, we’ll formally define Brownian motion, explore its properties, and begin developing the calculus of random functions.</p>
<!-- --- -->
<!-- ## For the Curious: Replicating the Simulations -->
<!-- While we've hidden the code to focus on concepts, here's how you could generate your own random walk simulations: -->
<!-- ```{r example_code, echo=TRUE, eval=FALSE} -->
<!-- # Generate one random walk path -->
<!-- set.seed(123) -->
<!-- n_steps <- 1000 -->
<!-- steps <- sample(c(-1, 1), size = n_steps, replace = TRUE) -->
<!-- position <- cumsum(steps) -->
<!-- # Plot -->
<!-- library(ggplot2) -->
<!-- ggplot(data.frame(time = 1:n_steps, position = position),  -->
<!--        aes(x = time, y = position)) + -->
<!--   geom_line(color = "blue") + -->
<!--   geom_hline(yintercept = 0, linetype = "dashed", color = "red") + -->
<!--   labs(title = "A Single Random Walk", -->
<!--        x = "Time (n)",  -->
<!--        y = "Position (Sₙ)") + -->
<!--   theme_minimal() -->
<!-- # For multiple paths -->
<!-- n_paths <- 5 -->
<!-- walks <- replicate(n_paths, cumsum(sample(c(-1, 1), n_steps, replace = TRUE))) -->
<!-- # Convert to long format for ggplot -->
<!-- library(tidyr) -->
<!-- walks_df <- as.data.frame(walks) %>% -->
<!--   mutate(time = 1:n_steps) %>% -->
<!--   pivot_longer(-time, names_to = "path", values_to = "position") -->
<!-- ggplot(walks_df, aes(x = time, y = position, color = path)) + -->
<!--   geom_line(alpha = 0.7) + -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs(title = "Five Independent Random Walk Paths", -->
<!--        x = "Time (n)",  -->
<!--        y = "Position (Sₙ)") + -->
<!--   theme_minimal() + -->
<!--   theme(legend.position = "none") -->
<!-- ``` -->
<!-- **Exercise:** Try modifying the code to create a biased random walk where $P(X_i = +1) = 0.6$ and $P(X_i = -1) = 0.4$. How do the properties change? What is $\mathbb{E}[S_n]$ now? -->
<!-- --- -->
<!-- ## Key Takeaways -->
<!-- Before moving to the next section, make sure you understand: -->
<!-- 1. **Definition:** A simple random walk is the cumulative sum of independent $\pm 1$ steps. -->
<!-- 2. **Moment formulas:** $\mathbb{E}[S_n] = 0$ and $\text{Var}(S_n) = n$ for symmetric steps. -->
<!-- 3. **Scaling behavior:** Typical deviations grow as $\sqrt{n}$, not $n$. -->
<!-- 4. **Central Limit Theorem:** $S_n / \sqrt{n} \xrightarrow{d} N(0,1)$ as $n \to \infty$. -->
<!-- 5. **Path to Brownian motion:** Continuous-time limits of scaled random walks yield Brownian motion. -->
<!-- If any of these points feel unclear, revisit the relevant section or work through the suggested exercises. These foundations are essential for everything that follows. -->
</div>
</div>
<div id="from-discrete-steps-to-continuous-paths" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> From Discrete Steps to Continuous Paths<a href="random-walks-and-brownian-motion.html#from-discrete-steps-to-continuous-paths" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ve thoroughly explored the simple random walk—a process that jumps at discrete integer times <span class="math inline">\(n = 1, 2, 3, \ldots\)</span>. But many real phenomena unfold continuously: pollen grains don’t wait for a clock to tick before changing direction, and stock prices can move at any instant during trading hours. How do we bridge this gap?</p>
<p>The key insight: we’ll take our discrete random walk and refine it twice—making steps <strong>more frequent</strong> and <strong>smaller</strong>—in just the right proportion so that something meaningful survives in the limit.</p>
<div id="the-scaling-thought-experiment" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> The Scaling Thought Experiment<a href="random-walks-and-brownian-motion.html#the-scaling-thought-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imagine we want to model a process over time interval <span class="math inline">\([0, t]\)</span>. We’ll compare three different strategies:</p>
<p><strong>Strategy 1 (Naive):</strong> Divide <span class="math inline">\([0, t]\)</span> into <span class="math inline">\(n\)</span> subintervals of length <span class="math inline">\(\Delta t = t/n\)</span>. At each subinterval, take a step of size <span class="math inline">\(\pm 1\)</span>.
- Position after <span class="math inline">\(n\)</span> steps: <span class="math inline">\(S_n \approx \sqrt{n} \sim \sqrt{t/\Delta t}\)</span>
- As <span class="math inline">\(\Delta t \to 0\)</span>: <span class="math inline">\(S_n \to \infty\)</span> (diverges!)</p>
<p><strong>Strategy 2 (Over-correction):</strong> Use step size <span class="math inline">\(\pm \Delta t\)</span> instead.
- Position: <span class="math inline">\(S_n \approx \sqrt{n} \cdot \Delta t \sim \sqrt{n/t^2} \to 0\)</span> (vanishes!)</p>
<p><strong>Strategy 3 (Goldilocks):</strong> Use step size <span class="math inline">\(\pm \sqrt{\Delta t}\)</span>.
- Position variance: <span class="math inline">\(n \cdot (\sqrt{\Delta t})^2 = n \cdot \Delta t = (t/\Delta t) \cdot \Delta t = t\)</span> ✓
- As <span class="math inline">\(\Delta t \to 0\)</span>: we get a non-trivial limit!</p>
<p>The third strategy is the magic formula. Let’s see why it works mathematically.</p>
</div>
<div id="the-crucial-scaling-derivation" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> The Crucial Scaling Derivation<a href="random-walks-and-brownian-motion.html#the-crucial-scaling-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we divide the time interval <span class="math inline">\([0, t]\)</span> into <span class="math inline">\(n\)</span> subintervals of length <span class="math inline">\(\Delta t = t/n\)</span>. At each time step, we take a random step of size <span class="math inline">\(\Delta x\)</span>.</p>
<p>After <span class="math inline">\(n\)</span> steps, our position is:
<span class="math display">\[S_n = \sum_{i=1}^{n} X_i\]</span>
where each <span class="math inline">\(X_i \in \{-\Delta x, +\Delta x\}\)</span> with equal probability.</p>
<p>We already know:
- <span class="math inline">\(\mathbb{E}[S_n] = 0\)</span> (regardless of <span class="math inline">\(\Delta x\)</span>)
- <span class="math inline">\(\text{Var}(S_n) = n \cdot (\Delta x)^2\)</span></p>
<p><strong>What should the variance be?</strong> If we want the limiting process to have variance proportional to elapsed time <span class="math inline">\(t\)</span> (as we’ve seen with random walks where <span class="math inline">\(\text{Var}(S_n) = n\)</span>), we need:
<span class="math display">\[\text{Var}(S_n) = n \cdot (\Delta x)^2 = t\]</span></p>
<p>Substituting <span class="math inline">\(n = t/\Delta t\)</span>:
<span class="math display">\[\frac{t}{\Delta t} \cdot (\Delta x)^2 = t\]</span></p>
<p>Solving for <span class="math inline">\(\Delta x\)</span>:
<span class="math display">\[(\Delta x)^2 = \Delta t \quad \Rightarrow \quad \boxed{\Delta x = \sqrt{\Delta t}}\]</span></p>
<p><strong>The profound implication:</strong> As we make time steps smaller by a factor of <span class="math inline">\(k\)</span> (i.e., <span class="math inline">\(\Delta t \to \Delta t/k\)</span>), we must make spatial steps smaller by a factor of <span class="math inline">\(\sqrt{k}\)</span> (i.e., <span class="math inline">\(\Delta x \to \Delta x/\sqrt{k}\)</span>). Steps shrink, but only like the <strong>square root</strong> of the time resolution.</p>
<blockquote>
<p><strong>💭 Intuition Check:</strong> Why square root scaling? Think back to our random walk property: <span class="math inline">\(\text{SD}(S_n) = \sqrt{n}\)</span>. When we compress <span class="math inline">\(n\)</span> steps into a fixed time interval <span class="math inline">\(t\)</span>, we need each step’s contribution to scale as <span class="math inline">\(1/\sqrt{n}\)</span> so that the total variance remains finite.</p>
</blockquote>
</div>
<div id="visualizing-the-convergence" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Visualizing the Convergence<a href="random-walks-and-brownian-motion.html#visualizing-the-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s see this convergence in action by simulating random walks with increasingly fine time discretizations:
<!-- Let's see this convergence in action: -->
<img src="stochastic-calculus_files/figure-html/brownian-convergence-animation-1.png" alt="" width="80%" style="display: block; margin: auto;" /></p>
<p><strong>What we observe:</strong>
- With <span class="math inline">\(n = 10\)</span> steps (left panel): The path is clearly discrete, with visible jumps.
- With <span class="math inline">\(n = 100\)</span> steps (middle panel): The path appears smoother but still jagged.
- With <span class="math inline">\(n = 10{,}000\)</span> steps (right panel): The path looks nearly continuous—this is Brownian motion emerging.</p>
<p>Each panel represents the <em>same</em> time interval <span class="math inline">\([0, 1]\)</span>, but with different levels of discretization. As <span class="math inline">\(n \to \infty\)</span> (and <span class="math inline">\(\Delta t \to 0\)</span>), these paths converge to a limiting continuous process.</p>
</div>
</div>
<div id="brownian-motion-the-continuous-limit" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Brownian Motion: The Continuous Limit<a href="random-walks-and-brownian-motion.html#brownian-motion-the-continuous-limit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="formal-definition" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Formal Definition<a href="random-walks-and-brownian-motion.html#formal-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Taking the limit as <span class="math inline">\(\Delta t \to 0\)</span> in our scaled random walk, we obtain a continuous stochastic process <span class="math inline">\(\{B(t) : t \geq 0\}\)</span> called <strong>standard Brownian motion</strong> (or the <strong>Wiener process</strong>) if it satisfies:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Starts at the origin:</strong> <span class="math inline">\(B(0) = 0\)</span></p></li>
<li><p><strong>Independent increments:</strong> For any times <span class="math inline">\(0 \leq t_1 &lt; t_2 &lt; t_3 &lt; t_4\)</span>, the increments <span class="math inline">\(B(t_2) - B(t_1)\)</span> and <span class="math inline">\(B(t_4) - B(t_3)\)</span> are independent.</p>
<p><em>Interpretation:</em> The process has no memory—what happens in disjoint time intervals is statistically independent.</p></li>
<li><p><strong>Stationary normal increments:</strong> For any <span class="math inline">\(0 \leq s &lt; t\)</span>,
<span class="math display">\[B(t) - B(s) \sim N(0, t-s)\]</span></p>
<p><em>Interpretation:</em> Over any time interval of length <span class="math inline">\(\tau = t - s\)</span>, the displacement is normally distributed with mean zero and variance <span class="math inline">\(\tau\)</span>. The distribution depends only on the <strong>length</strong> of the interval, not its position.</p></li>
<li><p><strong>Continuous paths:</strong> <span class="math inline">\(t \mapsto B(t)\)</span> is a continuous function with probability 1.</p>
<p><em>Interpretation:</em> Unlike the random walk, which jumps, Brownian motion flows continuously. You can draw its path without lifting your pen (though it would be infinitely wiggly!).</p></li>
</ol>
</div>
<div id="understanding-the-properties" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Understanding the Properties<a href="random-walks-and-brownian-motion.html#understanding-the-properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s unpack these conditions with concrete examples:</p>
<p><strong>Property 1 (Origin):</strong> This is a normalization. If we wanted <span class="math inline">\(B(t)\)</span> to start elsewhere, we’d simply add a constant: <span class="math inline">\(B(t) + x_0\)</span>.</p>
<p><strong>Property 2 (Independence):</strong> Suppose you observe <span class="math inline">\(B(t) = 5\)</span> at time <span class="math inline">\(t = 1\)</span>. What does this tell you about <span class="math inline">\(B(2) - B(1)\)</span>? <strong>Nothing!</strong> The increment is still <span class="math inline">\(N(0, 1)\)</span>, independent of where the process currently sits. This memoryless property is inherited from the random walk.</p>
<p><strong>Property 3 (Stationarity):</strong> The distribution of <span class="math inline">\(B(t) - B(s)\)</span> depends only on <span class="math inline">\(t - s\)</span>. So <span class="math inline">\(B(2) - B(1)\)</span> has the same distribution as <span class="math inline">\(B(102) - B(101)\)</span>—both are <span class="math inline">\(N(0, 1)\)</span>. But <span class="math inline">\(B(3) - B(1) \sim N(0, 2)\)</span> has larger variance because it spans a longer time interval.</p>
<p><strong>Property 4 (Continuity):</strong> This seems innocent but leads to shocking consequences, as we’ll see next.</p>
<blockquote>
<p><strong>🔗 Connection to Random Walk:</strong> Notice that if we evaluate Brownian motion at discrete times <span class="math inline">\(t = k \cdot \Delta t\)</span> for <span class="math inline">\(k = 0, 1, 2, \ldots\)</span>, the increments <span class="math inline">\(B(k\Delta t) - B((k-1)\Delta t)\)</span> are i.i.d. <span class="math inline">\(N(0, \Delta t)\)</span>. This is exactly a scaled random walk! Brownian motion is the continuous interpolation between these points.</p>
</blockquote>
</div>
<div id="an-important-consequence-quadratic-variation" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> An Important Consequence: Quadratic Variation<a href="random-walks-and-brownian-motion.html#an-important-consequence-quadratic-variation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From Property 3, we can immediately derive a key fact:</p>
<p><span class="math display">\[\mathbb{E}[B(t)] = 0 \quad \text{and} \quad \text{Var}(B(t)) = t\]</span></p>
<p>So <span class="math inline">\(B(t) \sim N(0, t)\)</span>, meaning:
<span class="math display">\[B(t) \overset{d}{=} \sqrt{t} \cdot Z\]</span>
where <span class="math inline">\(Z \sim N(0, 1)\)</span>.</p>
<p><strong>Physical interpretation:</strong> A particle undergoing Brownian motion has typical displacement <span class="math inline">\(\sim \sqrt{t}\)</span> after time <span class="math inline">\(t\)</span>. Its position grows sub-linearly—if you wait four times as long, you only expect to be twice as far from the origin (in terms of standard deviation).</p>
<p><strong>Financial interpretation (Bachelier, 1900):</strong> If stock price movements follow Brownian motion, the uncertainty in price grows as <span class="math inline">\(\sqrt{t}\)</span>. This means volatility (uncertainty per unit time) scales as <span class="math inline">\(1/\sqrt{t}\)</span>—longer time horizons provide more predictability per unit time.</p>
</div>
<div id="the-shocking-fact-nowhere-differentiable" class="section level3 hasAnchor" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> The Shocking Fact: Nowhere Differentiable<a href="random-walks-and-brownian-motion.html#the-shocking-fact-nowhere-differentiable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here’s the counterintuitive reality: <strong>Brownian motion is continuous everywhere but differentiable nowhere</strong> (with probability 1). It’s infinitely jagged at every time scale!</p>
<p><strong>Why this happens:</strong> Differentiability means:
<span class="math display">\[\lim_{\Delta t \to 0} \frac{B(t + \Delta t) - B(t)}{\Delta t} \text{ exists}\]</span></p>
<p>But we know <span class="math inline">\(B(t + \Delta t) - B(t) \sim N(0, \Delta t)\)</span>, which means:
<span class="math display">\[\frac{B(t + \Delta t) - B(t)}{\Delta t} \sim N\left(0, \frac{1}{\Delta t}\right)\]</span></p>
As <span class="math inline">\(\Delta t \to 0\)</span>, the variance of this ratio <strong>diverges</strong>! The “derivative” has infinite variance—it doesn’t exist in the classical sense.
<div class="figure" style="text-align: center">
<img src="stochastic-calculus_files/figure-html/nowhere_diff-1.png" alt="Zooming into a Brownian path. Each successive zoom (left to right) reveals the same rough, jagged structure—there are no smooth segments, no matter how close you look." width="80%" />
<p class="caption">
(#fig:nowhere_diff)Zooming into a Brownian path. Each successive zoom (left to right) reveals the same rough, jagged structure—there are no smooth segments, no matter how close you look.
</p>
</div>
<p><strong>What the figure shows:</strong> Three panels showing the same Brownian path at increasing magnifications around <span class="math inline">\(t = 0.5\)</span>. Notice that the path never smooths out—the jaggedness is <strong>self-similar</strong> at all scales. This is fundamentally different from smooth functions like <span class="math inline">\(\sin(t)\)</span> or <span class="math inline">\(t^2\)</span>, which look linear when you zoom in enough.</p>
<p><strong>Practical implication:</strong> You cannot use calculus in the usual sense. The expression <span class="math inline">\(dB(t)/dt\)</span> is meaningless. This is why we need <strong>stochastic calculus</strong>—a new calculus built specifically for these nowhere-differentiable paths.</p>
</div>
</div>
<div id="a-crisis-and-an-opportunity-rethinking-calculus" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> A Crisis and An Opportunity: Rethinking Calculus<a href="random-walks-and-brownian-motion.html#a-crisis-and-an-opportunity-rethinking-calculus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The nowhere-differentiability of Brownian motion isn’t just a mathematical curiosity—it represents a fundamental crisis for classical calculus and simultaneously opens the door to something new.</p>
<div id="why-classical-calculus-fails" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Why Classical Calculus Fails<a href="random-walks-and-brownian-motion.html#why-classical-calculus-fails" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classical calculus rests on a foundation of <strong>slopes</strong>. The derivative:
<span class="math display">\[f&#39;(t) = \lim_{\Delta t \to 0} \frac{f(t + \Delta t) - f(t)}{\Delta t}\]</span>
measures the instantaneous rate of change. From this single concept flows everything: tangent lines, optimization, differential equations, and integration (via the fundamental theorem).</p>
<p>But for Brownian motion, this foundation crumbles. We’ve seen that:
<span class="math display">\[\frac{B(t + \Delta t) - B(t)}{\Delta t} \sim N\left(0, \frac{1}{\Delta t}\right)\]</span></p>
<p>As <span class="math inline">\(\Delta t \to 0\)</span>, this ratio doesn’t converge to anything—its variance explodes to infinity. <strong>The derivative <span class="math inline">\(dB/dt\)</span> simply does not exist.</strong></p>
<p>This means:
- ❌ We cannot draw tangent lines to <span class="math inline">\(B(t)\)</span>
- ❌ We cannot use the chain rule in its classical form
- ❌ We cannot solve differential equations like <span class="math inline">\(\frac{dX}{dt} = \sigma B(t)\)</span> using ordinary methods
- ❌ The fundamental theorem of calculus doesn’t apply directly</p>
<p><strong>The verdict:</strong> Classical calculus is fundamentally incompatible with Brownian motion and other stochastic processes. We need a new calculus.</p>
</div>
<div id="the-key-insight-build-from-changes-not-slopes" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> The Key Insight: Build from Changes, Not Slopes<a href="random-walks-and-brownian-motion.html#the-key-insight-build-from-changes-not-slopes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here’s the conceptual breakthrough, first developed by Kiyoshi Itô in the 1940s:</p>
<blockquote>
<p><strong>Instead of building calculus from derivatives (rates of change), we’ll build it from differentials (increments of change).</strong></p>
</blockquote>
<p>In classical calculus, these two perspectives are equivalent via the differential:
<span class="math display">\[df = f&#39;(t) \, dt\]</span></p>
<p>But for stochastic processes, they’re not! While <span class="math inline">\(dB/dt\)</span> doesn’t exist, the <strong>increment</strong> <span class="math inline">\(dB\)</span> does exist and has a precise probabilistic meaning:</p>
<p><span class="math display">\[\boxed{dB_t \sim N(0, dt)}\]</span></p>
<p>This says: over an infinitesimal time interval of length <span class="math inline">\(dt\)</span>, the Brownian increment <span class="math inline">\(dB_t\)</span> is normally distributed with mean zero and variance <span class="math inline">\(dt\)</span>.</p>
<p><strong>Why this works:</strong> We’re no longer asking “what is the slope at time <span class="math inline">\(t\)</span>?” (which has no answer). Instead, we’re asking “what is the probability distribution of the change from <span class="math inline">\(t\)</span> to <span class="math inline">\(t + dt\)</span>?” (which does have an answer).</p>
<p>This is the seed from which all of stochastic calculus grows.</p>
</div>
<div id="the-arithmetic-of-infinitesimals-db2-neq-0" class="section level3 hasAnchor" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> The Arithmetic of Infinitesimals: <span class="math inline">\((dB)^2 \neq 0\)</span><a href="random-walks-and-brownian-motion.html#the-arithmetic-of-infinitesimals-db2-neq-0" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In classical calculus, when working with a smooth function <span class="math inline">\(f(t)\)</span>, we treat <span class="math inline">\((dt)^2\)</span> as negligible:
<span class="math display">\[(dt)^2 = o(dt) \approx 0\]</span></p>
<p>This is because <span class="math inline">\((0.01)^2 = 0.0001\)</span> is much smaller than <span class="math inline">\(0.01\)</span>. As <span class="math inline">\(dt \to 0\)</span>, the squared term vanishes faster.</p>
<p><strong>But Brownian motion breaks this rule.</strong> Let’s see why:</p>
<p>Consider the square of a Brownian increment:
<span class="math display">\[(dB_t)^2 = [B(t + dt) - B(t)]^2\]</span></p>
<p>Since <span class="math inline">\(dB_t \sim N(0, dt)\)</span>, we can write <span class="math inline">\(dB_t = \sqrt{dt} \cdot Z\)</span> where <span class="math inline">\(Z \sim N(0,1)\)</span>. Therefore:
<span class="math display">\[(dB_t)^2 = (\sqrt{dt} \cdot Z)^2 = dt \cdot Z^2\]</span></p>
<p>Taking expectations:
<span class="math display">\[\mathbb{E}[(dB_t)^2] = dt \cdot \mathbb{E}[Z^2] = dt \cdot 1 = dt\]</span></p>
<p><strong>The shocking conclusion:</strong> <span class="math inline">\((dB_t)^2\)</span> is not negligible—it’s of the same order as <span class="math inline">\(dt\)</span> itself!</p>
<p>In the language of stochastic calculus, we write:
<span class="math display">\[\boxed{(dB_t)^2 = dt}\]</span></p>
<p>This is a <strong>deterministic</strong> relationship (the randomness averages out), and it’s the signature rule of stochastic calculus.</p>
</div>
<div id="comparing-classical-and-stochastic-infinitesimals" class="section level3 hasAnchor" number="1.5.4">
<h3><span class="header-section-number">1.5.4</span> Comparing Classical and Stochastic Infinitesimals<a href="random-walks-and-brownian-motion.html#comparing-classical-and-stochastic-infinitesimals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s make the contrast explicit. For a smooth function <span class="math inline">\(x(t)\)</span> and Brownian motion <span class="math inline">\(B(t)\)</span>:</p>
<table>
<thead>
<tr class="header">
<th><strong>Classical Calculus</strong></th>
<th><strong>Stochastic Calculus</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(dx = x&#39;(t) \, dt\)</span> exists</td>
<td><span class="math inline">\(dB/dt\)</span> does not exist</td>
</tr>
<tr class="even">
<td><span class="math inline">\(dx \sim O(dt)\)</span></td>
<td><span class="math inline">\(dB \sim O(\sqrt{dt})\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\((dx)^2 = O(dt^2) \approx 0\)</span></td>
<td><span class="math inline">\((dB)^2 = dt \neq 0\)</span></td>
</tr>
<tr class="even">
<td>Paths are smooth</td>
<td>Paths are jagged</td>
</tr>
<tr class="odd">
<td>Finite variation</td>
<td>Infinite variation</td>
</tr>
</tbody>
</table>
<p>The third row is particularly striking. <strong>Squaring</strong> a classical differential makes it negligible. <strong>Squaring</strong> a stochastic differential leaves it the same order of magnitude.</p>
<blockquote>
<p><strong>💭 Heuristic Intuition:</strong> Think of <span class="math inline">\(dB\)</span> as being “roughly of size <span class="math inline">\(\sqrt{dt}\)</span>.” Then:
- Classical: <span class="math inline">\((dt)^2 \sim dt \cdot dt\)</span> (negligible)
- Stochastic: <span class="math inline">\((dB)^2 \sim (\sqrt{dt})^2 = dt\)</span> (non-negligible)</p>
<p>This <span class="math inline">\(\sqrt{dt}\)</span> vs. <span class="math inline">\(dt\)</span> distinction is what makes stochastic calculus fundamentally different.</p>
</blockquote>
</div>
<div id="quadratic-variation-a-new-concept" class="section level3 hasAnchor" number="1.5.5">
<h3><span class="header-section-number">1.5.5</span> Quadratic Variation: A New Concept<a href="random-walks-and-brownian-motion.html#quadratic-variation-a-new-concept" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This leads us to a notion that has no classical analog: <strong>quadratic variation</strong>.</p>
<p>For a smooth function <span class="math inline">\(f(t)\)</span> over <span class="math inline">\([0, T]\)</span>, partition the interval into <span class="math inline">\(n\)</span> pieces and compute:
<span class="math display">\[\text{QV}[f] = \lim_{n \to \infty} \sum_{i=1}^{n} [f(t_i) - f(t_{i-1})]^2 = 0\]</span></p>
<p>The sum vanishes because smooth functions have finite total variation (they don’t wiggle infinitely much).</p>
<p>But for Brownian motion:
<span class="math display">\[\text{QV}[B] = \lim_{n \to \infty} \sum_{i=1}^{n} [B(t_i) - B(t_{i-1})]^2 = T\]</span></p>
<p><strong>Interpretation:</strong> Even though each individual increment <span class="math inline">\(dB\)</span> is small (order <span class="math inline">\(\sqrt{dt}\)</span>), when you square them and add them up, you get something finite and deterministic—the elapsed time itself.</p>
<p>In integral notation:
<span class="math display">\[\int_0^T (dB_t)^2 = \int_0^T dt = T\]</span></p>
<p>This is written symbolically as:
<span class="math display">\[\langle B, B \rangle_T = T\]</span>
where <span class="math inline">\(\langle B, B \rangle\)</span> denotes the quadratic variation process.</p>
<div id="why-quadratic-variation-matters" class="section level4 hasAnchor" number="1.5.5.1">
<h4><span class="header-section-number">1.5.5.1</span> Why Quadratic Variation Matters<a href="random-walks-and-brownian-motion.html#why-quadratic-variation-matters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The non-zero quadratic variation of Brownian motion has profound consequences:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Itô’s Lemma:</strong> The stochastic chain rule will include a second-order correction term proportional to <span class="math inline">\((dB)^2 = dt\)</span>. This doesn’t appear in classical calculus because <span class="math inline">\((dx)^2 = 0\)</span> there.</p></li>
<li><p><strong>Energy and volatility:</strong> In physics, quadratic variation relates to energy dissipation. In finance, it measures the accumulated volatility of a price path.</p></li>
<li><p><strong>Distinguishing processes:</strong> Two processes might have the same first-order behavior but different quadratic variations—this difference can be economically meaningful (e.g., distinguishing continuous vs. jump processes).</p></li>
<li><p><strong>No arbitrage:</strong> In financial mathematics, the quadratic variation appears in the change of measure techniques that underpin option pricing.</p>
<div class="figure" style="text-align: center">
<img src="stochastic-calculus_files/figure-html/quad_var_demo-1.png" alt="Approximating quadratic variation. As partition gets finer (more points), the sum of squared increments converges to elapsed time $T$, not to zero." width="80%" />
<p class="caption">
(#fig:quad_var_demo)Approximating quadratic variation. As partition gets finer (more points), the sum of squared increments converges to elapsed time <span class="math inline">\(T\)</span>, not to zero.
</p>
</div></li>
</ol>
<p><strong>What the simulation shows:</strong> We partition <span class="math inline">\([0, 1]\)</span> into <span class="math inline">\(n = 10, 100, 1000, 10000\)</span> intervals and compute <span class="math inline">\(\sum [B(t_i) - B(t_{i-1})]^2\)</span>. The sum converges to 1 (the elapsed time), confirming that quadratic variation is non-zero and equals the time interval.</p>
</div>
</div>
<div id="a-new-foundation-for-calculus" class="section level3 hasAnchor" number="1.5.6">
<h3><span class="header-section-number">1.5.6</span> A New Foundation for Calculus<a href="random-walks-and-brownian-motion.html#a-new-foundation-for-calculus" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s summarize the paradigm shift:</p>
<p><strong>Classical calculus:</strong>
- Foundation: Derivative (slope)
- Infinitesimal: <span class="math inline">\(dx = f&#39;(t) \, dt\)</span>
- Second-order: <span class="math inline">\((dx)^2 = 0\)</span> (negligible)
- Integration: Area under curves
- Applies to: Smooth, differentiable functions</p>
<p><strong>Stochastic calculus:</strong>
- Foundation: Increment (change)
- Infinitesimal: <span class="math inline">\(dB_t \sim N(0, dt)\)</span>
- Second-order: <span class="math inline">\((dB)^2 = dt\)</span> (non-negligible!)
- Integration: Limit of Riemann-like sums with randomness
- Applies to: Continuous but non-differentiable paths</p>
<p>The absence of derivatives forces us to think differently, but it opens up a richer mathematical structure. We trade smoothness for randomness, and discover that randomness has its own beautiful calculus.</p>
<div id="the-road-ahead" class="section level4 hasAnchor" number="1.5.6.1">
<h4><span class="header-section-number">1.5.6.1</span> The Road Ahead<a href="random-walks-and-brownian-motion.html#the-road-ahead" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>With these foundations in place, we’re ready to build stochastic calculus:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Stochastic integrals:</strong> What does <span class="math inline">\(\int_0^t f(s) \, dB_s\)</span> mean when <span class="math inline">\(dB\)</span> is random?</p></li>
<li><p><strong>Itô’s lemma:</strong> How do we differentiate <span class="math inline">\(f(B_t, t)\)</span> when <span class="math inline">\(B_t\)</span> has no derivative?</p></li>
<li><p><strong>Stochastic differential equations:</strong> How do we solve <span class="math inline">\(dX_t = \mu(X_t) \, dt + \sigma(X_t) \, dB_t\)</span>?</p></li>
</ol>
<p>Each of these will leverage the fundamental property <span class="math inline">\((dB)^2 = dt\)</span> in essential ways. The “crisis” of non-differentiability becomes an opportunity to discover new mathematics.</p>
<p>As we’ll see, this isn’t just abstract theory—it’s the language in which modern finance, physics, and engineering describe uncertainty.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="itô-integral-and-itôs-lemma.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": false,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/abhinavananddwivedi/book/edit/main/01-brownian-motion.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["stochastic-calculus.pdf", "stochastic-calculus.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
